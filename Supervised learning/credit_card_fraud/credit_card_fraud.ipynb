{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content\n",
    "<font color='blue'><a href='#1'>1. Introduction</a></font> \n",
    "\n",
    "<font color='blue'><a href='#2'>2. Data Overview</a></font> \n",
    " \n",
    "<font color='blue'><a href='#3'>3. Logistic Regression</a></font> \n",
    "\n",
    "<font color='blue'><a href='#4'>4. KNN similarity model</a></font>\n",
    " \n",
    "<font color='blue'><a href='#5'>5. Random Forest</a></font>\n",
    " \n",
    "<font color='blue'><a href='#6'>6. SVM</a></font> \n",
    "\n",
    "<font color='blue'><a href='#7'>7. Gradient Boosting</a></font> \n",
    "\n",
    "<font color='blue'><a href='#8'>8. Ridge Classifier</a></font> \n",
    "\n",
    "<font color='blue'><a href='#9'>9. Bernoulli/Naive Bayesian classifier</a></font> \n",
    "\n",
    "<font color='blue'><a href='#10'>10. Neural Network: Tensorflow</a></font> \n",
    "\n",
    "<font color='blue'><a href='#11'>11. Anomaly Detection</a></font> \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<font size=\"6\"><a id='1'>1. Introduction</a></font> \n",
    "\n",
    "I will be investigating the dataset \"Credit Card Fraud\". The datasets contains transactions made by credit cards in September 2013 by european cardholders. This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.\n",
    "\n",
    "It contains only numerical input variables which are the result of a PCA transformation( due to confidentiality issues, they cannot provide the original features). Features V1, V2, ... V28 are the principal components obtained with PCA, the only features which have not been transformed with PCA are 'Time' and 'Amount'. Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature 'Amount' is the transaction Amount, this feature can be used for example-dependant cost-senstive learning. Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"6\"><a id='2'>2. Data Overview</a></font> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
      "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
      "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
      "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
      "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
      "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
      "\n",
      "         V8        V9  ...         V21       V22       V23       V24  \\\n",
      "0  0.098698  0.363787  ...   -0.018307  0.277838 -0.110474  0.066928   \n",
      "1  0.085102 -0.255425  ...   -0.225775 -0.638672  0.101288 -0.339846   \n",
      "2  0.247676 -1.514654  ...    0.247998  0.771679  0.909412 -0.689281   \n",
      "3  0.377436 -1.387024  ...   -0.108300  0.005274 -0.190321 -1.175575   \n",
      "4 -0.270533  0.817739  ...   -0.009431  0.798278 -0.137458  0.141267   \n",
      "\n",
      "        V25       V26       V27       V28  Amount  Class  \n",
      "0  0.128539 -0.189115  0.133558 -0.021053  149.62      0  \n",
      "1  0.167170  0.125895 -0.008983  0.014724    2.69      0  \n",
      "2 -0.327642 -0.139097 -0.055353 -0.059752  378.66      0  \n",
      "3  0.647376 -0.221929  0.062723  0.061458  123.50      0  \n",
      "4 -0.206010  0.502292  0.219422  0.215153   69.99      0  \n",
      "\n",
      "[5 rows x 31 columns]\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 284807 entries, 0 to 284806\n",
      "Data columns (total 31 columns):\n",
      "Time      284807 non-null float64\n",
      "V1        284807 non-null float64\n",
      "V2        284807 non-null float64\n",
      "V3        284807 non-null float64\n",
      "V4        284807 non-null float64\n",
      "V5        284807 non-null float64\n",
      "V6        284807 non-null float64\n",
      "V7        284807 non-null float64\n",
      "V8        284807 non-null float64\n",
      "V9        284807 non-null float64\n",
      "V10       284807 non-null float64\n",
      "V11       284807 non-null float64\n",
      "V12       284807 non-null float64\n",
      "V13       284807 non-null float64\n",
      "V14       284807 non-null float64\n",
      "V15       284807 non-null float64\n",
      "V16       284807 non-null float64\n",
      "V17       284807 non-null float64\n",
      "V18       284807 non-null float64\n",
      "V19       284807 non-null float64\n",
      "V20       284807 non-null float64\n",
      "V21       284807 non-null float64\n",
      "V22       284807 non-null float64\n",
      "V23       284807 non-null float64\n",
      "V24       284807 non-null float64\n",
      "V25       284807 non-null float64\n",
      "V26       284807 non-null float64\n",
      "V27       284807 non-null float64\n",
      "V28       284807 non-null float64\n",
      "Amount    284807 non-null float64\n",
      "Class     284807 non-null int64\n",
      "dtypes: float64(30), int64(1)\n",
      "memory usage: 67.4 MB\n",
      "None\n",
      "Time      0\n",
      "V1        0\n",
      "V2        0\n",
      "V3        0\n",
      "V4        0\n",
      "V5        0\n",
      "V6        0\n",
      "V7        0\n",
      "V8        0\n",
      "V9        0\n",
      "V10       0\n",
      "V11       0\n",
      "V12       0\n",
      "V13       0\n",
      "V14       0\n",
      "V15       0\n",
      "V16       0\n",
      "V17       0\n",
      "V18       0\n",
      "V19       0\n",
      "V20       0\n",
      "V21       0\n",
      "V22       0\n",
      "V23       0\n",
      "V24       0\n",
      "V25       0\n",
      "V26       0\n",
      "V27       0\n",
      "V28       0\n",
      "Amount    0\n",
      "Class     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(filepath_or_buffer = \"/home/micah/Desktop/data/creditcard.csv\")\n",
    "\n",
    "print df.head()\n",
    "print df.info()\n",
    "print df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like there's no missing value in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x648 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make the correlation matrix.\n",
    "cor = df.corr()\n",
    "\n",
    "# Set up the matplotlib figure.\n",
    "f, ax = plt.subplots(figsize=(12, 9))\n",
    "\n",
    "# Draw the heatmap using seaborn.\n",
    "sns.heatmap(cor, vmax=.8, square=True)\n",
    "plt.show()\n",
    "# Heatmaps ar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the correlation between the independent variables or the features are low and that is a good sign!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"6\"><a id='3'>3. Logistic regression</a></font> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.003914\n",
      "         Iterations 13\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                  Class   No. Observations:               284807\n",
      "Model:                          Logit   Df Residuals:                   284776\n",
      "Method:                           MLE   Df Model:                           30\n",
      "Date:                Sun, 16 Jun 2019   Pseudo R-squ.:                  0.6922\n",
      "Time:                        15:25:19   Log-Likelihood:                -1114.8\n",
      "converged:                       True   LL-Null:                       -3621.2\n",
      "                                        LLR p-value:                     0.000\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Time       -3.742e-06   2.26e-06     -1.659      0.097   -8.16e-06    6.79e-07\n",
      "V1             0.0960      0.042      2.264      0.024       0.013       0.179\n",
      "V2             0.0094      0.058      0.161      0.872      -0.104       0.123\n",
      "V3            -0.0079      0.053     -0.149      0.881      -0.112       0.096\n",
      "V4             0.6986      0.074      9.454      0.000       0.554       0.843\n",
      "V5             0.1295      0.067      1.944      0.052      -0.001       0.260\n",
      "V6            -0.1198      0.074     -1.626      0.104      -0.264       0.025\n",
      "V7            -0.0969      0.067     -1.453      0.146      -0.228       0.034\n",
      "V8            -0.1739      0.030     -5.711      0.000      -0.234      -0.114\n",
      "V9            -0.2843      0.111     -2.561      0.010      -0.502      -0.067\n",
      "V10           -0.8176      0.097     -8.432      0.000      -1.008      -0.628\n",
      "V11           -0.0621      0.081     -0.762      0.446      -0.222       0.098\n",
      "V12            0.0909      0.087      1.045      0.296      -0.080       0.261\n",
      "V13           -0.3312      0.082     -4.058      0.000      -0.491      -0.171\n",
      "V14           -0.5571      0.062     -8.949      0.000      -0.679      -0.435\n",
      "V15           -0.1141      0.086     -1.330      0.183      -0.282       0.054\n",
      "V16           -0.1908      0.125     -1.526      0.127      -0.436       0.054\n",
      "V17           -0.0216      0.070     -0.309      0.757      -0.159       0.116\n",
      "V18           -0.0131      0.129     -0.102      0.919      -0.266       0.240\n",
      "V19            0.0963      0.097      0.993      0.321      -0.094       0.286\n",
      "V20           -0.4582      0.082     -5.607      0.000      -0.618      -0.298\n",
      "V21            0.3898      0.060      6.494      0.000       0.272       0.507\n",
      "V22            0.6297      0.134      4.707      0.000       0.367       0.892\n",
      "V23           -0.0951      0.058     -1.629      0.103      -0.209       0.019\n",
      "V24            0.1289      0.147      0.874      0.382      -0.160       0.418\n",
      "V25           -0.0761      0.131     -0.582      0.560      -0.332       0.180\n",
      "V26            0.0195      0.190      0.103      0.918      -0.352       0.392\n",
      "V27           -0.8188      0.122     -6.686      0.000      -1.059      -0.579\n",
      "V28           -0.2937      0.088     -3.332      0.001      -0.467      -0.121\n",
      "Amount         0.0009      0.000      2.449      0.014       0.000       0.002\n",
      "intercept     -8.3917      0.249    -33.652      0.000      -8.880      -7.903\n",
      "==============================================================================\n",
      "\n",
      "Possibly complete quasi-separation: A fraction 0.31 of observations can be\n",
      "perfectly predicted. This might indicate that there is complete\n",
      "quasi-separation. In this case some parameters will not be identified.\n"
     ]
    }
   ],
   "source": [
    "# Statsmodel logistic regression\n",
    "\n",
    "# Declare predictors.\n",
    "y_statsmod = df['Class']\n",
    "x_statsmod = df.iloc[:,:-1]\n",
    "\n",
    "# The Statsmodels formulation requires a column with constant value 1 that\n",
    "# will act as the intercept.\n",
    "x_statsmod['intercept'] = 1 \n",
    "\n",
    "# Declare and fit the model.\n",
    "logit = sm.Logit(y_statsmod, x_statsmod)\n",
    "result = logit.fit()\n",
    "\n",
    "# Lots of information about the model and its coefficients, but the\n",
    "# accuracy rate for predictions is missing.\n",
    "print(result.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0         6.299447e-04\n",
      "1         2.003029e-04\n",
      "2         3.495181e-04\n",
      "3         1.472313e-04\n",
      "4         1.971930e-04\n",
      "5         1.449841e-04\n",
      "6         5.519510e-04\n",
      "7         4.667054e-04\n",
      "8         2.374541e-04\n",
      "9         1.146105e-04\n",
      "10        5.964671e-05\n",
      "11        1.901861e-04\n",
      "12        3.955599e-05\n",
      "13        1.513142e-03\n",
      "14        1.687237e-03\n",
      "15        1.043697e-04\n",
      "16        6.497464e-04\n",
      "17        1.429813e-04\n",
      "18        1.279607e-03\n",
      "19        4.343146e-05\n",
      "20        2.532104e-04\n",
      "21        5.933660e-04\n",
      "22        6.902076e-04\n",
      "23        9.459918e-04\n",
      "24        1.238256e-05\n",
      "25        2.246580e-04\n",
      "26        4.770405e-04\n",
      "27        2.099362e-04\n",
      "28        1.391865e-03\n",
      "29        5.939646e-04\n",
      "              ...     \n",
      "284777    3.198327e-04\n",
      "284778    4.871661e-04\n",
      "284779    2.272604e-04\n",
      "284780    2.089247e-04\n",
      "284781    6.702510e-05\n",
      "284782    3.937880e-03\n",
      "284783    1.345799e-05\n",
      "284784    2.303508e-04\n",
      "284785    2.022098e-05\n",
      "284786    9.456946e-06\n",
      "284787    2.331267e-04\n",
      "284788    1.017093e-04\n",
      "284789    5.359694e-04\n",
      "284790    1.311487e-04\n",
      "284791    1.835383e-04\n",
      "284792    2.130599e-04\n",
      "284793    9.550190e-05\n",
      "284794    1.350181e-04\n",
      "284795    9.886743e-13\n",
      "284796    6.964171e-04\n",
      "284797    9.703805e-05\n",
      "284798    4.004162e-04\n",
      "284799    1.893514e-04\n",
      "284800    9.427192e-05\n",
      "284801    1.119752e-04\n",
      "284802    9.632559e-10\n",
      "284803    1.815906e-04\n",
      "284804    2.247533e-04\n",
      "284805    4.330019e-04\n",
      "284806    2.974124e-04\n",
      "Length: 284807, dtype: float64\n",
      "\n",
      " Accuracy by admission status\n",
      "col_0       0    1\n",
      "Class             \n",
      "0      284273   42\n",
      "1         184  308\n",
      "\n",
      " Percentage accuracy\n",
      "284807\n",
      "0.9992064801778047\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy. First, get probability that each row will be a fraud.\n",
    "pred_statsmod = result.predict(x_statsmod)\n",
    "print pred_statsmod\n",
    "\n",
    "# fraud 1 if probability is greater than .5.\n",
    "pred_statsmod = np.where(pred_statsmod < .5, 0, 1)\n",
    "\n",
    "# Accuracy table.\n",
    "table = pd.crosstab(y_statsmod, pred_statsmod)\n",
    "\n",
    "print('\\n Accuracy by admission status')\n",
    "print(table)\n",
    "print('\\n Percentage accuracy')\n",
    "print table.sum().sum()#.iloc[0]\n",
    "print((table.iloc[0,0] + table.iloc[1,1]) / float(table.sum().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients\n",
      "[[-7.12206575e-05  3.18682793e-01 -4.83850387e-01 -7.93127563e-01\n",
      "   1.20289540e-01  5.75021683e-02 -5.40495940e-02  3.35171136e-01\n",
      "  -3.74165961e-01 -3.88332949e-01 -2.06986480e-01 -2.86517603e-01\n",
      "   1.85123052e-02 -3.06438278e-01 -6.94224243e-01 -4.27600151e-01\n",
      "  -2.94579466e-01 -4.39702486e-01  3.10746972e-02  2.64950391e-02\n",
      "   9.19504147e-02  2.48760049e-01  3.50865605e-01  6.76883576e-02\n",
      "  -2.44272764e-02 -3.56044928e-01  6.07018132e-02 -8.87962032e-02\n",
      "   2.77870551e-02 -5.58303923e-03]]\n",
      "Intercept\n",
      "[-1.62808759]\n",
      "\n",
      " Accuracy by admission status\n",
      "\n",
      "Class       0    1\n",
      "row_0             \n",
      "0      284240  203\n",
      "1          75  289\n",
      "\n",
      " Percentage accuracy\n",
      "\n",
      "0.9990239003957065\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    284443\n",
      "           1       0.59      0.79      0.68       364\n",
      "\n",
      "   micro avg       1.00      1.00      1.00    284807\n",
      "   macro avg       0.79      0.90      0.84    284807\n",
      "weighted avg       1.00      1.00      1.00    284807\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#SKlearn logistic regression functionality includes a parameter regularization element that penalizes extremely\n",
    "# large parameters in the name of increasing predictive accuracy.\n",
    "lr = LogisticRegression()\n",
    "\n",
    "y = df['Class']\n",
    "X = df.iloc[:,:-1]\n",
    "\n",
    "fit = lr.fit(X, y)\n",
    "\n",
    "print('Coefficients')\n",
    "print(fit.coef_)\n",
    "print('Intercept')\n",
    "print(fit.intercept_)\n",
    "pred_y = lr.predict(X)\n",
    "\n",
    "print('\\n Accuracy by admission status\\n')\n",
    "print(pd.crosstab(pred_y, y))\n",
    "\n",
    "print('\\n Percentage accuracy\\n')\n",
    "print(lr.score(X, y))\n",
    "\n",
    "print(classification_report(pred_y, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frauds account for 0.172% \n",
    "\n",
    "In case if we wanted to use regression to predict a categorical variable with more than two possible outcomes, we use multinomial logistic regression and LogisticRegression from SKlearn can handle multiple classes out-of-the-box.\n",
    "If want ordinal logistic regression like marathon runners places from 1st to 100th, we can use python MORD package. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-7-7c4249d5a25f>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-7-7c4249d5a25f>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    https://towardsdatascience.com/building-a-logistic-regression-in-python-step-by-step-becd4d56c9c8\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "https://towardsdatascience.com/building-a-logistic-regression-in-python-step-by-step-becd4d56c9c8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"6\"><a id='4'>4. KNN similarity</a></font> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNeighborsClassifier can take longer than other models. So with big data, it can become a bit more time consuming. \n",
    "K nearest observations are not all similarly close to the test. In that case it may be useful to weight by distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbors = KNeighborsClassifier(n_neighbors=1)\n",
    "\n",
    "df1 = df.loc[:float(len(df)/100),:]\n",
    "print 'for simplicity reason, we chose 1% of the original data for k neighbours'\n",
    "\n",
    "y = df1['Class']\n",
    "X = df1.iloc[:,:-1]\n",
    "neighbors.fit(X,y)\n",
    "\n",
    "pred= neighbors.predict(X)\n",
    "\n",
    "# fraud 1 if probability is greater than .5.\n",
    "#pred_statsmod = np.where(pred_statsmod < .5, 0, 1)\n",
    "\n",
    "# Accuracy table.\n",
    "table = pd.crosstab(y, pred)\n",
    "\n",
    "print('\\n Accuracy')\n",
    "print(table)\n",
    "print('\\n Percentage accuracy')\n",
    "print table.sum().sum()\n",
    "print((table.iloc[0,0] + table.iloc[1,1]) / float(table.sum().sum()))\n",
    "\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "score = cross_val_score(neighbors, X, y, cv=4)\n",
    "\n",
    "print(\"Unweighted Accuracy: %0.2f (+/- %0.2f)\" % (score.mean(), score.std()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbors2 = KNeighborsClassifier(n_neighbors=5,  weights='distance')\n",
    "\n",
    "neighbors2.fit(X,y)\n",
    "pred2= neighbors2.predict(X)\n",
    "table2 = pd.crosstab(y, pred2)\n",
    "\n",
    "score2 = cross_val_score(neighbors2, X, y, cv=4)\n",
    "print(\"Unweighted Accuracy: %0.2f (+/- %0.2f)\" % (score2.mean(), score2.std()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it is a good idea to represent the outcome in the mesh. However, because the fraud rate is so low compared to the non-fraud cases and the fact that it is computationally expensive, it is not a good idea in this specific case. I have tried to do it and I've got a MemoryError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbors = KNeighborsClassifier(n_neighbors=2, weights='distance')\n",
    "df1 = df.loc[:float(len(df)/500),:]\n",
    "#print 'for simplicity reason, we chose 1% of the original data for k neighbours meshing'\n",
    "\n",
    "y = df1['Class']\n",
    "X = df1.iloc[:,:-1]\n",
    "\n",
    "neighbors.fit(X, y)\n",
    "\n",
    "X = np.array(X)\n",
    "Y = np.array(y)\n",
    "\n",
    "# Mesh size.\n",
    "h = .01\n",
    "\n",
    "# Plot the decision boundary. We assign a color to each point in the mesh.\n",
    "x_min = X[:,0].min() - .5\n",
    "x_max = X[:,0].max() + .5\n",
    "y_min = X[:,1].min() - .5\n",
    "y_max = X[:,1].max() + .5\n",
    "xx, yy = np.meshgrid(\n",
    "    np.arange(x_min, x_max, h),\n",
    "    np.arange(y_min, y_max, h)\n",
    ")\n",
    "print len(xx.ravel() )\n",
    "print len(yy.ravel() )\n",
    "Z = neighbors.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.figure(1, figsize=(6, 4))\n",
    "plt.set_cmap(plt.cm.Paired)\n",
    "plt.pcolormesh(xx, yy, Z)\n",
    "\n",
    "# Add the training points to the plot.\n",
    "plt.scatter(X[:, 0], X[:, 1], c=Y)\n",
    "plt.xlabel('')\n",
    "plt.ylabel('')\n",
    "\n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.ylim(yy.min(), yy.max())\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"6\"><a id='5'>5. Random Forest</a></font> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees\n",
    "With Decision Tree, we ask a series of questions to gain information about the data, narrowing down the possible number of questions each time. To be efficient, we use Entropy(disorder) and here it will mean uncertainty. It can be defined, using Shannon Entropy \n",
    "\n",
    "$$ H = -\\sum_{i=1}^n P(x_i)log_2 P(x_i) $$\n",
    "\n",
    "So the idea of which node need to be explored more is based on entropy number. With the entropy score of 1 being the most uncertain, 0 being totally certain, we decide what we explore more to mazximize information gain. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the model we'll be using.\n",
    "from sklearn import tree\n",
    "\n",
    "\n",
    "# A convenience for displaying visualizations.\n",
    "from IPython.display import Image\n",
    "# Packages for rendering our tree.\n",
    "\n",
    "import pydotplus\n",
    "import graphviz\n",
    "\n",
    "customers = pd.DataFrame()\n",
    "customers['purchases_amount'] = [105, 65, 89, 99, 149, 102, 34, 120, 129, 39,20, 30, 109, 40, 55, 100, 23, 20, 70, 10]\n",
    "customers['purchases_items'] = [1, 4, 5, 4, 7, 1, 2, 10, 6, 5,\n",
    "                                1, 3, 2, 1, 5, 10, 3, 3, 1, 1]\n",
    "customers['promo'] = [1, 1, 0, 1, 0, 0, 0, 0, 0, 1,\n",
    "                      1, 1, 1, 0, 1, 1, 1, 0, 1, 1]\n",
    "customers['email_list'] = [1, 0, 1, 1, 1, 0, 1, 1, 1, 1,\n",
    "                           0, 1, 1, 0, 1, 0, 1, 1, 0, 0]\n",
    "customers['checkouts'] = [1, 5, 3, 3, 1, 2, 4, 4, 1, 1,\n",
    "                          1, 1, 2, 4, 1, 1, 2, 1, 1, 1]\n",
    "\n",
    "repeat_customer = pd.DataFrame()\n",
    "repeat_customer['repeat'] = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
    "                             0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "print 'Here is an example of rendering tree. '\n",
    "\n",
    "# Initialize and train our tree.\n",
    "decision_tree = tree.DecisionTreeClassifier(\n",
    "    criterion='entropy',\n",
    "    max_features=1,\n",
    "    max_depth=4,\n",
    "    random_state = 1337\n",
    ")\n",
    "decision_tree.fit(customers, repeat_customer)\n",
    "\n",
    "# Render our tree.\n",
    "dot_data = tree.export_graphviz(\n",
    "    decision_tree, out_file=None,\n",
    "    feature_names=customers.columns,\n",
    "    class_names=['Not Returning', 'Returning'],\n",
    "    filled=True\n",
    ")\n",
    "graph = pydotplus.graph_from_dot_data(dot_data)\n",
    "Image(graph.create_png())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train our tree.\n",
    "y = df['Class']\n",
    "X = df.iloc[:,:-1]\n",
    "decision_tree = tree.DecisionTreeClassifier(\n",
    "    criterion='entropy',\n",
    "    max_features=1,\n",
    "    max_depth=4,\n",
    "    random_state = 1337\n",
    ")\n",
    "decision_tree.fit(X, y)\n",
    "\n",
    "# Render our tree.\n",
    "dot_data = tree.export_graphviz(\n",
    "    decision_tree, out_file=None,\n",
    "    feature_names=X.columns,\n",
    "    class_names=['Not Returning', 'Returning'],\n",
    "    filled=True\n",
    ")\n",
    "graph = pydotplus.graph_from_dot_data(dot_data)\n",
    "Image(graph.create_png())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is not a hard and fast rule to how the tree is built, so it doesn't build the same way every time. You saw this above when we discussed the random_state argument. In addition, they are incredibly prone to overfitting, particularly if you allow them to grow too deep or complex. Also note that because they are working from information gain, they are biased towards the dominant class, so balanced data is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import ensemble\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "rfc = ensemble.RandomForestClassifier()\n",
    "\n",
    "score = cross_val_score(rfc, X, y, cv=4)\n",
    "print (score)\n",
    "print(\"Unweighted Accuracy: %0.2f (+/- %0.2f)\" % (score.mean(), score2.std()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"6\"><a id='6'>6. Support Vector Machine</a></font> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " In SVM, all the classes are bounded by a line! Boundary line passing close to data points is bad because it'll be susceptable to noise and be worse at predicting new data points than a boundary line farther away. The margin as the distance between the nearest point of each class in the boundary. The goal of SVM is find the best boundary, or the boundary that optimizes the margin.\n",
    " \n",
    " SVM's primary advantage is its flexibility. It can have great visual explanatory power (linear SVC), tremendous accuracy (kernel smoothing), clustering (SVClustering), or the ability to control the specificity of training (SVR). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm = SVC(kernel = 'linear')\n",
    "svm.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize our model\n",
    "y_min, y_max = X.test.min() - 1, X.test.max() + 3\n",
    "x_min, x_max = X.project.min() - 1, X.project.max() + 3\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, .1),\n",
    "                     np.arange(y_min, y_max, .1))\n",
    "\n",
    "Z = (svm.predict(np.c_[xx.ravel(), yy.ravel()])=='pass')\n",
    "\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.8)\n",
    "plt.scatter(test_data.project[0:10], test_data.test[0:10], marker='x')\n",
    "plt.scatter(test_data.project[10:20], test_data.test[10:20], marker='o')\n",
    "plt.xlim(x_min, x_max)\n",
    "plt.ylim(y_min, y_max)\n",
    "plt.xlabel('Project Grade')\n",
    "plt.ylabel('Test Grade')\n",
    "plt.title('Passing Grades SVM Example')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.scatter(y, svr.predict(X))\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "print cross_val_score(svr, X, y, cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"6\"><a id='7'>7. Gradient Boosting</a></font> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boosting Models, models the data over and over, each time adjusting the model based on what was learned from the previous one. The principle behind boosting is iterative. We start by fitting a simple model on all the data. We identify the information that the model was not able to account for (incorrect predictions in classifier, and residuals in regression) and build a new simple model that targets that new pool of information. We repeat this until we reach some predetermined stopping rule.  The combination of all the models is then used to make the final predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The regression model is fitted by moving down the steepest 'downhill' gradient until we reach the lowest point of the surface, where all possible gradients are 'uphill.' The final model is made up of the parameter estimates that define that location on the surface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss function to minimize is the sum of the squared residuals:\n",
    "\n",
    "$$\\frac1{n}\\sum_{i=1}^n(y_i-(\\alpha + \\beta x_i))^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import ensemble\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import scipy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training and test sets.\n",
    "offset = int(X.shape[0] * 0.9)\n",
    "\n",
    "# Put 90% of the data in the training set.\n",
    "X_train, y_train = X[:offset], y[:offset]\n",
    "\n",
    "# And put 10% in the test set.\n",
    "X_test, y_test = X[offset:], y[offset:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll make 500 iterations, use 2-deep trees, and set our loss function.\n",
    "params = {'n_estimators': 500,\n",
    "          'max_depth': 2,\n",
    "          'loss': 'deviance'}\n",
    "\n",
    "# Initialize and fit the model.\n",
    "clf = ensemble.GradientBoostingClassifier(**params)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "predict_train = clf.predict(X_train)\n",
    "predict_test = clf.predict(X_test)\n",
    "\n",
    "# Accuracy tables.\n",
    "table_train = pd.crosstab(y_train, predict_train, margins=True)\n",
    "table_test = pd.crosstab(y_test, predict_test, margins=True)\n",
    "\n",
    "train_tI_errors = table_train.loc[0.0,1.0] / table_train.loc['All','All']\n",
    "train_tII_errors = table_train.loc[1.0,0.0] / table_train.loc['All','All']\n",
    "\n",
    "test_tI_errors = table_test.loc[0.0,1.0]/table_test.loc['All','All']\n",
    "test_tII_errors = table_test.loc[1.0,0.0]/table_test.loc['All','All']\n",
    "\n",
    "print((\n",
    "    'Training set accuracy:\\n'\n",
    "    'Percent Type I errors: {}\\n'\n",
    "    'Percent Type II errors: {}\\n\\n'\n",
    "    'Test set accuracy:\\n'\n",
    "    'Percent Type I errors: {}\\n'\n",
    "    'Percent Type II errors: {}'\n",
    ").format(train_tI_errors, train_tII_errors, test_tI_errors, test_tII_errors))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = clf.feature_importances_\n",
    "\n",
    "# Make importances relative to max importance.\n",
    "feature_importance = 100.0 * (feature_importance / feature_importance.max())\n",
    "sorted_idx = np.argsort(feature_importance)\n",
    "pos = np.arange(sorted_idx.shape[0]) + .5\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.barh(pos, feature_importance[sorted_idx], align='center')\n",
    "plt.yticks(pos, X.columns[sorted_idx])\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.title('Variable Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"6\"><a id='8'>8. Ridge Classifier</a></font> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ridge regression** minimizes this cost function:\n",
    "\n",
    "$$\\sum_{i=1}^n(y_i-(\\alpha+\\beta x_i))^2+\\lambda\\sum_{j=1}^p\\beta_j^2 $$\n",
    "\n",
    "Comparing this cost function to the OLS cost function above, you can see it consists of the OLS function with a new part to the right:\n",
    "\n",
    "$$\\lambda\\sum_{j=1}^p\\beta_j^2 $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr1 = linear_model.LinearRegression()\n",
    "#Y_train = #df_train['income'].values.reshape(-1, 1)\n",
    "#X_train = df_train.loc[:, ~(df_train.columns).isin(['income'])]\n",
    "regr1.fit(X, y)\n",
    "print('\\nR-squared simple model:')\n",
    "print(regr1.score(X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridgeregr = linear_model.RidgeClassifier(alpha=10, fit_intercept=True) \n",
    "ridgeregr.fit(X, y)\n",
    "print(ridgeregr.score(X, y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"6\"><a id='9'>9. Naive Bayesian</a></font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our data is binary / boolean, so we're importing the Bernoulli classifier.\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "# Instantiate our model and store it in a new variable.\n",
    "bnb = BernoulliNB()\n",
    "\n",
    "# Fit our model to the data.\n",
    "bnb.fit(X, y)\n",
    "\n",
    "print(bnb.score(X, y))\n",
    "\n",
    "'''\n",
    "# Classify, storing the result in a new variable.\n",
    "y_pred = bnb.predict(data)\n",
    "\n",
    "# Display our results.\n",
    "print(\"Number of mislabeled points out of a total {} points : {}\".format(\n",
    "    data.shape[0],\n",
    "    (target != y_pred).sum()\n",
    "))\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
