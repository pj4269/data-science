{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "latex_envs": {
      "LaTeX_envs_menu_present": true,
      "autoclose": false,
      "autocomplete": true,
      "bibliofile": "biblio.bib",
      "cite_by": "apalike",
      "current_citInitial": 1,
      "eqLabelWithNumbers": true,
      "eqNumInitial": 1,
      "hotkeys": {
        "equation": "Ctrl-E",
        "itemize": "Ctrl-I"
      },
      "labels_anchors": false,
      "latex_user_defs": false,
      "report_style_numbering": false,
      "user_envs_cfg": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "name": "4.solution_activation_loss_functions.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1_JqS_6bLr_",
        "colab_type": "text"
      },
      "source": [
        "# Solutions for checkpoint 4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7g9ieakbLsK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential \n",
        "from tensorflow.keras.layers import Dense"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RmSXIxu2bLsk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "input_dim = 784  # 28*28\n",
        "output_dim = nb_classes = 10\n",
        "batch_size = 128\n",
        "nb_epoch = 20\n",
        "\n",
        "X_train = X_train.reshape(60000, input_dim)\n",
        "X_test = X_test.reshape(10000, input_dim)\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "X_train /= 255\n",
        "X_test /= 255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uMbKcZFIbLs1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Y_train = to_categorical(y_train, nb_classes)\n",
        "Y_test = to_categorical(y_test, nb_classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-lMrXVybLtH",
        "colab_type": "text"
      },
      "source": [
        "## 1. In this task, you'll implement several ANN models with different activation functions. Specifically:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0oXP9YVbLtN",
        "colab_type": "text"
      },
      "source": [
        "### Implement a three layer ANN model with 128, 64 and 10 neurons in the layers using tanh activation function for each layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "na4AFxxpbLtT",
        "colab_type": "code",
        "colab": {},
        "outputId": "a29a95bc-a6c9-4222-e762-82dafcbc948c"
      },
      "source": [
        "model = Sequential()\n",
        "# our first dense layer\n",
        "model.add(Dense(128, input_shape=(784,), activation=\"tanh\"))\n",
        "# our second dense layer\n",
        "model.add(Dense(64, activation=\"tanh\"))\n",
        "# last layer is the output layer.\n",
        "model.add(Dense(10, activation=\"softmax\"))\n",
        "\n",
        "model.compile(optimizer='sgd', loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# setting verbose=1 prints out some results after each epoch\n",
        "model.fit(X_train, Y_train, batch_size=batch_size, epochs=20, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /Users/mladmin/miniconda3/envs/datascience/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /Users/mladmin/miniconda3/envs/datascience/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Epoch 1/20\n",
            "60000/60000 [==============================] - 1s 22us/sample - loss: 1.0228 - acc: 0.7473\n",
            "Epoch 2/20\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 0.5206 - acc: 0.8713\n",
            "Epoch 3/20\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 0.4206 - acc: 0.8896\n",
            "Epoch 4/20\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 0.3729 - acc: 0.8995\n",
            "Epoch 5/20\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 0.3436 - acc: 0.9051\n",
            "Epoch 6/20\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.3228 - acc: 0.9100\n",
            "Epoch 7/20\n",
            "60000/60000 [==============================] - 1s 18us/sample - loss: 0.3067 - acc: 0.9136\n",
            "Epoch 8/20\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.2937 - acc: 0.9171\n",
            "Epoch 9/20\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 0.2824 - acc: 0.9198\n",
            "Epoch 10/20\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 0.2726 - acc: 0.92260s - loss: 0.2794 - \n",
            "Epoch 11/20\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 0.2640 - acc: 0.9248\n",
            "Epoch 12/20\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.2560 - acc: 0.9271\n",
            "Epoch 13/20\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 0.2488 - acc: 0.9293\n",
            "Epoch 14/20\n",
            "60000/60000 [==============================] - 1s 22us/sample - loss: 0.2420 - acc: 0.9309\n",
            "Epoch 15/20\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 0.2357 - acc: 0.9329\n",
            "Epoch 16/20\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 0.2297 - acc: 0.9352\n",
            "Epoch 17/20\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 0.2241 - acc: 0.9366\n",
            "Epoch 18/20\n",
            "60000/60000 [==============================] - 1s 22us/sample - loss: 0.2186 - acc: 0.9384\n",
            "Epoch 19/20\n",
            "60000/60000 [==============================] - 1s 22us/sample - loss: 0.2136 - acc: 0.9398\n",
            "Epoch 20/20\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.2087 - acc: 0.9413\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x10ae2b2b0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxcHuvJcbLti",
        "colab_type": "code",
        "colab": {},
        "outputId": "a626c0d8-c02f-43ec-8faf-181a352dd819"
      },
      "source": [
        "score = model.evaluate(X_test, Y_test, verbose=0)\n",
        "print('Test score:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test score: 0.20581834660768508\n",
            "Test accuracy: 0.9423\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9WkALnPbLtv",
        "colab_type": "text"
      },
      "source": [
        "### Implement a three layer ANN model with 128, 64 and 10 neurons in the layers using sigmoid activation function for each layer.\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6hwgKfMCbLtz",
        "colab_type": "code",
        "colab": {},
        "outputId": "8a20e0b2-e711-45f3-a267-07303c58fbb2"
      },
      "source": [
        "model = Sequential()\n",
        "# our first dense layer\n",
        "model.add(Dense(128, input_shape=(784,), activation=\"sigmoid\"))\n",
        "# our second dense layer\n",
        "model.add(Dense(64, activation=\"sigmoid\"))\n",
        "# last layer is the output layer.\n",
        "model.add(Dense(10, activation=\"softmax\"))\n",
        "\n",
        "model.compile(optimizer='sgd', loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# setting verbose=1 prints out some results after each epoch\n",
        "model.fit(X_train, Y_train, batch_size=batch_size, epochs=20, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 2.2970 - acc: 0.1375\n",
            "Epoch 2/20\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 2.2386 - acc: 0.3223\n",
            "Epoch 3/20\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 2.1797 - acc: 0.4586\n",
            "Epoch 4/20\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 2.0979 - acc: 0.5424\n",
            "Epoch 5/20\n",
            "60000/60000 [==============================] - 1s 18us/sample - loss: 1.9811 - acc: 0.5875\n",
            "Epoch 6/20\n",
            "60000/60000 [==============================] - 1s 18us/sample - loss: 1.8232 - acc: 0.6102\n",
            "Epoch 7/20\n",
            "60000/60000 [==============================] - 1s 18us/sample - loss: 1.6353 - acc: 0.6439\n",
            "Epoch 8/20\n",
            "60000/60000 [==============================] - 1s 18us/sample - loss: 1.4445 - acc: 0.6734\n",
            "Epoch 9/20\n",
            "60000/60000 [==============================] - 1s 18us/sample - loss: 1.2761 - acc: 0.7064\n",
            "Epoch 10/20\n",
            "60000/60000 [==============================] - 1s 18us/sample - loss: 1.1395 - acc: 0.7325\n",
            "Epoch 11/20\n",
            "60000/60000 [==============================] - 1s 18us/sample - loss: 1.0313 - acc: 0.7564\n",
            "Epoch 12/20\n",
            "60000/60000 [==============================] - 1s 18us/sample - loss: 0.9447 - acc: 0.7768\n",
            "Epoch 13/20\n",
            "60000/60000 [==============================] - 1s 18us/sample - loss: 0.8736 - acc: 0.7919\n",
            "Epoch 14/20\n",
            "60000/60000 [==============================] - 1s 18us/sample - loss: 0.8139 - acc: 0.8051\n",
            "Epoch 15/20\n",
            "60000/60000 [==============================] - 1s 18us/sample - loss: 0.7628 - acc: 0.8176\n",
            "Epoch 16/20\n",
            "60000/60000 [==============================] - 1s 18us/sample - loss: 0.7185 - acc: 0.8275\n",
            "Epoch 17/20\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.6798 - acc: 0.8368\n",
            "Epoch 18/20\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.6456 - acc: 0.8440\n",
            "Epoch 19/20\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.6154 - acc: 0.8508\n",
            "Epoch 20/20\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.5886 - acc: 0.8559\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x1295c3860>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ExKlwDbEbLt_",
        "colab_type": "code",
        "colab": {},
        "outputId": "5e166a3c-0ef9-4353-b20b-de699accb12d"
      },
      "source": [
        "score = model.evaluate(X_test, Y_test, verbose=0)\n",
        "print('Test score:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test score: 0.5598974136829377\n",
            "Test accuracy: 0.863\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fnSNQ0xbLuY",
        "colab_type": "text"
      },
      "source": [
        "### Implement a three layer ANN model with 128, 64 and 10 neurons in the layers using ReLU activation function for each layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pOXcwGerbLub",
        "colab_type": "code",
        "colab": {},
        "outputId": "8c39479b-9c6c-4907-e11f-04586a87adaa"
      },
      "source": [
        "model = Sequential()\n",
        "# our first dense layer\n",
        "model.add(Dense(128, input_shape=(784,), activation=\"relu\"))\n",
        "# our second dense layer\n",
        "model.add(Dense(64, activation=\"relu\"))\n",
        "# last layer is the output layer.\n",
        "model.add(Dense(10, activation=\"softmax\"))\n",
        "\n",
        "model.compile(optimizer='sgd', loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# setting verbose=1 prints out some results after each epoch\n",
        "model.fit(X_train, Y_train, batch_size=batch_size, epochs=20, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 1.2379 - acc: 0.6780\n",
            "Epoch 2/20\n",
            "60000/60000 [==============================] - 1s 18us/sample - loss: 0.5055 - acc: 0.8667\n",
            "Epoch 3/20\n",
            "60000/60000 [==============================] - 1s 18us/sample - loss: 0.3917 - acc: 0.8912\n",
            "Epoch 4/20\n",
            "60000/60000 [==============================] - 1s 18us/sample - loss: 0.3436 - acc: 0.9024\n",
            "Epoch 5/20\n",
            "60000/60000 [==============================] - 1s 18us/sample - loss: 0.3140 - acc: 0.9106\n",
            "Epoch 6/20\n",
            "60000/60000 [==============================] - 1s 17us/sample - loss: 0.2926 - acc: 0.9163\n",
            "Epoch 7/20\n",
            "60000/60000 [==============================] - 1s 18us/sample - loss: 0.2753 - acc: 0.9217\n",
            "Epoch 8/20\n",
            "60000/60000 [==============================] - 1s 18us/sample - loss: 0.2607 - acc: 0.9250\n",
            "Epoch 9/20\n",
            "60000/60000 [==============================] - 1s 18us/sample - loss: 0.2482 - acc: 0.9293\n",
            "Epoch 10/20\n",
            "60000/60000 [==============================] - 1s 18us/sample - loss: 0.2373 - acc: 0.9320\n",
            "Epoch 11/20\n",
            "60000/60000 [==============================] - 1s 18us/sample - loss: 0.2275 - acc: 0.9355\n",
            "Epoch 12/20\n",
            "60000/60000 [==============================] - 1s 18us/sample - loss: 0.2186 - acc: 0.9375\n",
            "Epoch 13/20\n",
            "60000/60000 [==============================] - 1s 18us/sample - loss: 0.2103 - acc: 0.9396\n",
            "Epoch 14/20\n",
            "60000/60000 [==============================] - 1s 18us/sample - loss: 0.2033 - acc: 0.9422\n",
            "Epoch 15/20\n",
            "60000/60000 [==============================] - 1s 18us/sample - loss: 0.1963 - acc: 0.9449\n",
            "Epoch 16/20\n",
            "60000/60000 [==============================] - 1s 18us/sample - loss: 0.1901 - acc: 0.9464\n",
            "Epoch 17/20\n",
            "60000/60000 [==============================] - 1s 18us/sample - loss: 0.1841 - acc: 0.9478\n",
            "Epoch 18/20\n",
            "60000/60000 [==============================] - 1s 18us/sample - loss: 0.1786 - acc: 0.9499\n",
            "Epoch 19/20\n",
            "60000/60000 [==============================] - 1s 18us/sample - loss: 0.1735 - acc: 0.9510\n",
            "Epoch 20/20\n",
            "60000/60000 [==============================] - 1s 18us/sample - loss: 0.1684 - acc: 0.9527\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x13538b898>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g0FBmAQFbLuq",
        "colab_type": "code",
        "colab": {},
        "outputId": "fc7bad60-a862-4803-a97f-b54a35896f75"
      },
      "source": [
        "score = model.evaluate(X_test, Y_test, verbose=0)\n",
        "print('Test score:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test score: 0.17043185496479274\n",
            "Test accuracy: 0.9502\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3wIj4pKobLu-",
        "colab_type": "text"
      },
      "source": [
        "### Compare the result of each model with each other. Which activation function did perform better?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6ZHwn2UbLvC",
        "colab_type": "text"
      },
      "source": [
        "The highest accuracies in both the training and the test sets are achieved using the ReLU function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5nz9As0bLvJ",
        "colab_type": "text"
      },
      "source": [
        "## 2. In this task, you'll implement the ANN models specified below with the hinge loss function as their loss functions:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FSbMFO3SbLvO",
        "colab_type": "text"
      },
      "source": [
        "### Implement a three layer ANN model with 128, 64 and 10 neurons in the layers using tanh activation function for each layer.\n",
        "   "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3_owlCzbLvP",
        "colab_type": "code",
        "colab": {},
        "outputId": "8b43e115-a184-4976-801b-68cecd2c8442"
      },
      "source": [
        "model = Sequential()\n",
        "# our first dense layer\n",
        "model.add(Dense(128, input_shape=(784,), activation=\"tanh\"))\n",
        "# our second dense layer\n",
        "model.add(Dense(64, activation=\"tanh\"))\n",
        "# last layer is the output layer.\n",
        "model.add(Dense(10, activation=\"softmax\"))\n",
        "\n",
        "model.compile(optimizer='sgd', loss='categorical_hinge',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# setting verbose=1 prints out some results after each epoch\n",
        "model.fit(X_train, Y_train, batch_size=batch_size, epochs=20, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /Users/mladmin/miniconda3/envs/datascience/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n",
            "Epoch 1/20\n",
            "60000/60000 [==============================] - 1s 22us/sample - loss: 0.9903 - acc: 0.4577\n",
            "Epoch 2/20\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.7698 - acc: 0.7298\n",
            "Epoch 3/20\n",
            "60000/60000 [==============================] - 1s 18us/sample - loss: 0.5670 - acc: 0.8071\n",
            "Epoch 4/20\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 0.4495 - acc: 0.8494\n",
            "Epoch 5/20\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 0.3799 - acc: 0.8667\n",
            "Epoch 6/20\n",
            "60000/60000 [==============================] - 1s 22us/sample - loss: 0.3367 - acc: 0.87730s - loss: 0.343\n",
            "Epoch 7/20\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.3075 - acc: 0.8838\n",
            "Epoch 8/20\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.2862 - acc: 0.88950s - loss: 0.2907 - acc\n",
            "Epoch 9/20\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.2698 - acc: 0.8939\n",
            "Epoch 10/20\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 0.2567 - acc: 0.8979\n",
            "Epoch 11/20\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.2459 - acc: 0.9009\n",
            "Epoch 12/20\n",
            "60000/60000 [==============================] - 1s 18us/sample - loss: 0.2368 - acc: 0.9040\n",
            "Epoch 13/20\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 0.2289 - acc: 0.9064\n",
            "Epoch 14/20\n",
            "60000/60000 [==============================] - 1s 22us/sample - loss: 0.2221 - acc: 0.9084\n",
            "Epoch 15/20\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.2161 - acc: 0.9105\n",
            "Epoch 16/20\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.2106 - acc: 0.9123\n",
            "Epoch 17/20\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.2058 - acc: 0.91360s - loss: 0.2079 - ac\n",
            "Epoch 18/20\n",
            "60000/60000 [==============================] - 1s 18us/sample - loss: 0.2012 - acc: 0.9155\n",
            "Epoch 19/20\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 0.1972 - acc: 0.9168\n",
            "Epoch 20/20\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.1933 - acc: 0.9183\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x136081a58>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7rUFOKNbLve",
        "colab_type": "code",
        "colab": {},
        "outputId": "f1688834-a287-43ab-ea85-f70487354ac5"
      },
      "source": [
        "score = model.evaluate(X_test, Y_test, verbose=0)\n",
        "print('Test score:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test score: 0.1860966409087181\n",
            "Test accuracy: 0.922\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ai5Gva5HbLvr",
        "colab_type": "text"
      },
      "source": [
        "### Implement a three layer ANN model with 128, 64 and 10 neurons in the layers using sigmoid activation function for each layer.\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u8Z2m5BkbLvw",
        "colab_type": "code",
        "colab": {},
        "outputId": "4555ccdc-1134-4d17-b172-8a768a0346b7"
      },
      "source": [
        "model = Sequential()\n",
        "# our first dense layer\n",
        "model.add(Dense(128, input_shape=(784,), activation=\"sigmoid\"))\n",
        "# our second dense layer\n",
        "model.add(Dense(64, activation=\"sigmoid\"))\n",
        "# last layer is the output layer.\n",
        "model.add(Dense(10, activation=\"softmax\"))\n",
        "\n",
        "model.compile(optimizer='sgd', loss='categorical_hinge',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# setting verbose=1 prints out some results after each epoch\n",
        "model.fit(X_train, Y_train, batch_size=batch_size, epochs=20, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "60000/60000 [==============================] - 1s 22us/sample - loss: 1.0201 - acc: 0.1113\n",
            "Epoch 2/20\n",
            "60000/60000 [==============================] - 1s 22us/sample - loss: 1.0068 - acc: 0.14210s - loss: 1.0082 - a - ETA: 0s - loss: 1.0072 - acc: 0.\n",
            "Epoch 3/20\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 1.0027 - acc: 0.1726\n",
            "Epoch 4/20\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 1.0022 - acc: 0.2194\n",
            "Epoch 5/20\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 1.0018 - acc: 0.2748\n",
            "Epoch 6/20\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 1.0014 - acc: 0.3288\n",
            "Epoch 7/20\n",
            "60000/60000 [==============================] - 1s 18us/sample - loss: 1.0010 - acc: 0.3791\n",
            "Epoch 8/20\n",
            "60000/60000 [==============================] - 1s 18us/sample - loss: 1.0006 - acc: 0.4281\n",
            "Epoch 9/20\n",
            "60000/60000 [==============================] - 1s 18us/sample - loss: 1.0003 - acc: 0.4673\n",
            "Epoch 10/20\n",
            "60000/60000 [==============================] - 1s 18us/sample - loss: 1.0000 - acc: 0.5016\n",
            "Epoch 11/20\n",
            "60000/60000 [==============================] - 1s 18us/sample - loss: 0.9997 - acc: 0.5300\n",
            "Epoch 12/20\n",
            "60000/60000 [==============================] - 1s 18us/sample - loss: 0.9995 - acc: 0.55490s - loss: 0.9\n",
            "Epoch 13/20\n",
            "60000/60000 [==============================] - 1s 18us/sample - loss: 0.9992 - acc: 0.5764\n",
            "Epoch 14/20\n",
            "60000/60000 [==============================] - 1s 18us/sample - loss: 0.9989 - acc: 0.5945\n",
            "Epoch 15/20\n",
            "60000/60000 [==============================] - 1s 18us/sample - loss: 0.9987 - acc: 0.6097\n",
            "Epoch 16/20\n",
            "60000/60000 [==============================] - 1s 18us/sample - loss: 0.9985 - acc: 0.6230\n",
            "Epoch 17/20\n",
            "60000/60000 [==============================] - 1s 18us/sample - loss: 0.9982 - acc: 0.63510s - loss: 0.9982 - acc: 0.6\n",
            "Epoch 18/20\n",
            "60000/60000 [==============================] - 1s 18us/sample - loss: 0.9980 - acc: 0.6454\n",
            "Epoch 19/20\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.9978 - acc: 0.6543\n",
            "Epoch 20/20\n",
            "60000/60000 [==============================] - 1s 18us/sample - loss: 0.9975 - acc: 0.6619\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x135d5e6a0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ORiJlr94bLv4",
        "colab_type": "code",
        "colab": {},
        "outputId": "eb451183-37f0-4057-ae6d-e0adec1f7e00"
      },
      "source": [
        "score = model.evaluate(X_test, Y_test, verbose=0)\n",
        "print('Test score:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test score: 0.9972937772750855\n",
            "Test accuracy: 0.6791\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7h8pX_MebLwA",
        "colab_type": "text"
      },
      "source": [
        "### Implement a three layer ANN model with 128, 64 and 10 neurons in the layers using ReLU activation function for each layer.\n",
        "   "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFcmRtwHbLwC",
        "colab_type": "code",
        "colab": {},
        "outputId": "381f9d3d-be5c-4d63-cb2d-58fb810c0845"
      },
      "source": [
        "model = Sequential()\n",
        "# our first dense layer\n",
        "model.add(Dense(128, input_shape=(784,), activation=\"relu\"))\n",
        "# our second dense layer\n",
        "model.add(Dense(64, activation=\"relu\"))\n",
        "# last layer is the output layer.\n",
        "model.add(Dense(10, activation=\"softmax\"))\n",
        "\n",
        "model.compile(optimizer='sgd', loss='categorical_hinge',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# setting verbose=1 prints out some results after each epoch\n",
        "model.fit(X_train, Y_train, batch_size=batch_size, epochs=20, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 1.0131 - acc: 0.3226\n",
            "Epoch 2/20\n",
            "60000/60000 [==============================] - 1s 18us/sample - loss: 0.9804 - acc: 0.5610\n",
            "Epoch 3/20\n",
            "60000/60000 [==============================] - 1s 18us/sample - loss: 0.8974 - acc: 0.6470\n",
            "Epoch 4/20\n",
            "60000/60000 [==============================] - 1s 18us/sample - loss: 0.7062 - acc: 0.7256\n",
            "Epoch 5/20\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.5146 - acc: 0.8270\n",
            "Epoch 6/20\n",
            "60000/60000 [==============================] - 1s 18us/sample - loss: 0.3895 - acc: 0.86840s - loss: 0.4028 - acc:\n",
            "Epoch 7/20\n",
            "60000/60000 [==============================] - 1s 18us/sample - loss: 0.3205 - acc: 0.88210s - loss: 0.3294 - ac\n",
            "Epoch 8/20\n",
            "60000/60000 [==============================] - 1s 18us/sample - loss: 0.2827 - acc: 0.8902\n",
            "Epoch 9/20\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 0.2588 - acc: 0.8956\n",
            "Epoch 10/20\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.2419 - acc: 0.9001\n",
            "Epoch 11/20\n",
            "60000/60000 [==============================] - 1s 18us/sample - loss: 0.2293 - acc: 0.9044\n",
            "Epoch 12/20\n",
            "60000/60000 [==============================] - 1s 18us/sample - loss: 0.2191 - acc: 0.9068\n",
            "Epoch 13/20\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.2106 - acc: 0.9099\n",
            "Epoch 14/20\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.2035 - acc: 0.9124\n",
            "Epoch 15/20\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.1973 - acc: 0.9144\n",
            "Epoch 16/20\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.1917 - acc: 0.9164\n",
            "Epoch 17/20\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.1870 - acc: 0.9181\n",
            "Epoch 18/20\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.1824 - acc: 0.9199\n",
            "Epoch 19/20\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.1783 - acc: 0.9213\n",
            "Epoch 20/20\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.1745 - acc: 0.9229\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x137052f60>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-U4DWqwbLwN",
        "colab_type": "code",
        "colab": {},
        "outputId": "cce75988-6f62-4ef8-8f7e-ee4abe378460"
      },
      "source": [
        "score = model.evaluate(X_test, Y_test, verbose=0)\n",
        "print('Test score:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test score: 0.16739646133184433\n",
            "Test accuracy: 0.9261\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuC2PvFxbLwc",
        "colab_type": "text"
      },
      "source": [
        "### Compare the result of each model with the result of the same model from the previous task. Which loss function did perform better?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tVAdgQS-bLwg",
        "colab_type": "text"
      },
      "source": [
        "The highest accuracies in both the training and the test sets are achieved using the ReLU function. Moreover, all accuracies for all the models are lower when we train our models using hinge loss in comparison to using cross entropy loss."
      ]
    }
  ]
}
