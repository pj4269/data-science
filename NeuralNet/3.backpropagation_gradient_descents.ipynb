{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective function\n",
    "\n",
    "In deep learning models, training is done using optimization via Gradient Descent. As in any optimization problem, we need to have an **objective function** and our goal would be to minimize or maximize it. In deep neural networks, the objective function is the very loss function we discussed in the previous checkpoint. Hence:\n",
    "\n",
    "> Training a deep neural network means minimizing its loss function with respect to its parameters.\n",
    "\n",
    "Therefore, when we're talking about training a neural network, we talk about minimizing the loss function by playing with the weights and the biases of the model. Optimization is a very broad area but we'll not go into more details of it. Let's jump into the gradient descent algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient descent\n",
    "\n",
    "Gradient descent is an iterative method of solving optimization problems. In our case, we need to minimize the loss function with respect to the parameters of the network - that is the weights and the biases in all of the layers. \n",
    "\n",
    "Before talking about the algorithm itself, let's first understand what **gradient** is. In mathematics, gradient is defined as the first derivative of function with respect to a variable. If we denote our loss function as $L(x)$ where $x$ is a parameter to be optimized, then the gradient of $L$ with respect to $x$ is denoted by $L'(x)$. Yes, it's the very first order derivative of $L$ with respect to $x$. \n",
    "\n",
    "More generally, we can consider $W$ as a vector such that each element represents a parameter of the network: $\\boldsymbol{W} = [w_1, w_2, \\ldots, w_d]^\\top$. \n",
    "\n",
    "Then, the gradient of the loss function $L(\\boldsymbol{W})$ with respect to $\\boldsymbol{W}$ is a vector consisting of $d$ partial derivatives:\n",
    "\n",
    "$$\\nabla_{\\boldsymbol{W}} L(\\boldsymbol{W}) = \\bigg[\\frac{\\partial L(\\boldsymbol{W})}{\\partial w_1}, \\frac{\\partial L(\\boldsymbol{W})}{\\partial w_2}, \\ldots, \\frac{\\partial L(\\boldsymbol{W})}{\\partial w_d}\\bigg]^\\top.$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The algorithm\n",
    "\n",
    "The gradient descent algorithm belongs to the family of iterative algorithms. It starts with initial values (often random assignments or zero) for the parameters to be optimized. Then in each iteration, it updates the values of the parameters according to a rule. In the following, we give the gradient descent algorithm:\n",
    "\n",
    "1. Initialize the parameters with some values: \n",
    "\n",
    "$$W=W_0$$\n",
    "\n",
    "That is, \n",
    "$$w_1 = w_{10},$$\n",
    "$$w_2 = w_{20},$$\n",
    "$$.$$\n",
    "$$.$$\n",
    "$$w_d = w_{d0}.$$\n",
    "\n",
    "2. At every step t, update each parameter according to the following rule:\n",
    "\n",
    "$$W_{t+1} \\leftarrow W_{t} - \\eta L'(W_{t})$$\n",
    "\n",
    "That is, \n",
    "$$w_{1t+1} = w_{1t} - \\eta L'(w_{1t}),$$ \n",
    "$$w_{2t+1} = w_{2t} - \\eta L'(w_{2t}),$$\n",
    "$$.$$\n",
    "$$.$$\n",
    "$$w_{dt+1} = w_{dt} - \\eta L'(w_{dt}).$$\n",
    "\n",
    "3. The iteration stops if $L(W_{t+1}) - L(W_{t}) < \\epsilon$ or $t=T$. That is, if the differences between the consecutive values of the objective (loss) function is below a threshold or the iteration number reached to a preset threshold T.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The learning rate\n",
    "\n",
    "Note that in the gradient descent algorithm, the **learning rate** parameter $\\eta$ is something that is determined outside of the model. That is to say, **the learning rate is a hyperparameter of the model to be tuned**. Setting the learning rate to a value too high or too low may have serious consequences. If we set the learning rate to a good value, two things should happen:\n",
    "\n",
    "1. The training should converge.\n",
    "2. The convergence speed should be reasonable.\n",
    "\n",
    "The figure below illustrates this:\n",
    "\n",
    "![Good learning rate](assets/good_learning_rate.png)\n",
    "\n",
    "On the other hand, if we set the learning rate too high, we risk of divergence. That's the training process will never converge. The figure below shows this:\n",
    "\n",
    "![High learning rate](assets/high_learning_rate.png)\n",
    "\n",
    "Last, if we set the learning rate too low, our training process should converge very slowly as shown below:\n",
    "\n",
    "![Low learning rate](assets/low_learning_rate.png)\n",
    "\n",
    "As a result, we should be careful at setting the learning rate to a reasonable value. The default values in the Keras might be a good starting point for many of the tasks if you don't have any guiding idea on what value to set the learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variants of the gradient descent\n",
    "\n",
    "So far, we discussed a general form of the gradient descent algorithm. However, there are many variants of it. Next, we'll mention a few of them.\n",
    "\n",
    "### Stochastic gradient descent (SGD)\n",
    "\n",
    "In deep learning, the objective function is usually the average of the loss functions for each example in the training data set. We assume that $L_i(\\boldsymbol{W})$ is the loss function of the training data with $n$ examples. Here $\\boldsymbol{W}$ denotes the parameter vector, then we have the objective function:\n",
    "\n",
    "$$L(\\boldsymbol{W}) = \\frac{1}{n} \\sum_{i = 1}^n L_i(\\boldsymbol{W})$$\n",
    "\n",
    "The gradient of the objective function at $\\boldsymbol{W}$ is computed as\n",
    "\n",
    "$$\\nabla L(\\boldsymbol{x}) = \\frac{1}{n} \\sum_{i = 1}^n \\nabla L_i(\\boldsymbol{W}).$$\n",
    "\n",
    "If gradient descent is used, the computing cost for each independent variable iteration is $\\mathcal{O}(n)$, which grows linearly with $n$. Therefore, when the model training data instance is large, the cost of gradient descent for each iteration will be very high.\n",
    "\n",
    "Stochastic gradient descent (SGD) reduces computational cost at each iteration. At each iteration of stochastic gradient descent, we uniformly sample an index $i\\in\\{1,\\ldots,n\\}$ for data instances at random, and compute the gradient $\\nabla f_i(\\boldsymbol{W})$ to update $\\boldsymbol{W}$:\n",
    "\n",
    "$$\\boldsymbol{W} \\leftarrow \\boldsymbol{W} - \\eta \\nabla f_i(\\boldsymbol{W}).$$\n",
    "\n",
    "Here, $\\eta$ is the learning rate. We can see that the computing cost for each iteration drops from $\\mathcal{O}(n)$ of the gradient descent to the constant $\\mathcal{O}(1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini-batch stochastic gradient descent\n",
    "\n",
    "In each iteration, the gradient descent uses the entire training data set to compute the gradient, so it is sometimes referred to as **batch gradient descent**. Stochastic gradient descent (SGD) only randomly select one example in each iteration to compute the gradient. Just like in the previous chapters, we can perform random uniform sampling for each iteration to form a mini-batch and then use this mini-batch to compute the gradient.\n",
    "\n",
    "If you recall, this is what we did when we trained ANN models in the previous checkpoints. When we're training our models using Keras, we set the size of the mini batch as follows:\n",
    "\n",
    "```python\n",
    "model.fit(X_train, Y_train, batch_size=batch_size, epochs=20, verbose=1)\n",
    "```\n",
    "\n",
    "In the code above, `batch_size` parameter refers to the mini batch size of the gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation\n",
    "\n",
    "We've learned how we train our neural networks. However, one problem needs to be answered. A deep neural network has many layers. How can we take the derivative of the loss function with respect to the weights of, say, the first layer? Or more formally, how can we compute the gradient of the loss function with respect to the previous layers' weights?\n",
    "\n",
    "The method used in computing the gradients in a deep neural network is called **backpropagation**. Consider the following figure. It depicts a simple neural network with two hidden layers:\n",
    "\n",
    "![backpropagation](assets/backpropagation.png)\n",
    "\n",
    "In the figure, the loss function is a direct function of the weights w6 and w7. Hence, calculating the gradients of the loss with respect to w6 and w7 is simple. We just take the first derivative of the loss with respect to w6 and w7:\n",
    "\n",
    "$$\\frac{\\partial L(\\boldsymbol{W})}{\\partial w_6}, \\frac{\\partial L(\\boldsymbol{W})}{\\partial w_7}$$\n",
    "\n",
    "But, how do we calculate the gradient of the loss function with respect to w3, w4 and w5? The answer is we can apply the chain rule since w6 is a function of w3, w4 and w5:\n",
    "\n",
    "$$\\frac{\\partial L(\\boldsymbol{W})}{\\partial w_5} = \\frac{\\partial L(\\boldsymbol{W})}{\\partial w_6} \\frac{\\partial w6}{\\partial w_5}$$\n",
    "\n",
    "$$\\frac{\\partial L(\\boldsymbol{W})}{\\partial w_4} = \\frac{\\partial L(\\boldsymbol{W})}{\\partial w_6} \\frac{\\partial w6}{\\partial w_4}$$\n",
    "\n",
    "$$\\frac{\\partial L(\\boldsymbol{W})}{\\partial w_3} = \\frac{\\partial L(\\boldsymbol{W})}{\\partial w_6} \\frac{\\partial w6}{\\partial w_3}$$\n",
    "\n",
    "Similarly, we can calculate the gradients of the loss function with respect to the w1 and w2 using the chain rule as follows:\n",
    "\n",
    "$$\\frac{\\partial L(\\boldsymbol{W})}{\\partial w_2} = \\frac{\\partial L(\\boldsymbol{W})}{\\partial w_6} \\frac{\\partial w6}{\\partial w_3} \\frac{\\partial w3}{\\partial w_2}$$\n",
    "\n",
    "$$\\frac{\\partial L(\\boldsymbol{W})}{\\partial w_1} = \\frac{\\partial L(\\boldsymbol{W})}{\\partial w_6} \\frac{\\partial w6}{\\partial w_3} \\frac{\\partial w3}{\\partial w_1}$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
