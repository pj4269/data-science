{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installing TensorFlow and Keras\n",
    "\n",
    "Before showing steps of installation, it is important to make note of the computation units on your machine that TensorFlow would use. You have two options to run your TensorFlow code: \n",
    "\n",
    "1. You can use the CPU or \n",
    "2. You can use the GPU. \n",
    "\n",
    "Since GPU's are more suitable to run linear matrix operations faster than the CPUs, data scientists prefer to use them when available. Although training deep learning models on GPUs are way faster than training them on CPUs, accessing GPUs might be costly. The easiest solution for training your models on a GPU is using Google Colaboratory which offers free GPUs. That being said, the TensorFlow codes we'll write will be the same regardless of the underlying computational unit.\n",
    "\n",
    "Letâ€™s start with the installation of the TensorFlow. In doing so, we make use of the pip package manager of Python. If you'd like to install CPU version of the TensorFlow, then just run this on your terminal:\n",
    "\n",
    "```bash\n",
    "pip install tensorflow==2.0.0-rc1\n",
    "```\n",
    "\n",
    "However, if you'd like to use GPU version of the TensorFlow, you should run the following:\n",
    "\n",
    "```bash\n",
    "pip install tensorflow-gpu==2.0.0-rc1\n",
    "```\n",
    "\n",
    "Once you installed TensorFlow, Keras will come bundled to it. So, we don't need to install Keras separately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basics of Keras\n",
    "\n",
    "Keras offers three different ways of defining and running a deep learning model. These are known as:\n",
    "\n",
    "* The sequential api,\n",
    "* The functional api and\n",
    "* Model subclassing.\n",
    "\n",
    "Throughout this module, we'll make use of the **sequential api** since it offers the easiest way to define and run a deep learning model. The other two apis enable us to write some more sophisticated deep learning architectures. But, for our purposes the sequential api offers more than enough. We import it in our code as the following:\n",
    "\n",
    "```python\n",
    "from tensorflow.keras.models import Sequential \n",
    "```\n",
    "\n",
    "As we'll see shortly, using `Sequential()` class, we'll build our ANN step by step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing a basic ANN model\n",
    "\n",
    "Building a deep learning model using the sequential api of Keras is quite easy. Remember that the deep learning models consist of layers stacked on top of each other. For example, the second layer comes after the first layer, the third layer comes after the second layer etc. Keras mimics this sequential structure of the deep learning model when implementing them programmatically.\n",
    "\n",
    "When implementing an ANN in Keras, we do followings step by step:\n",
    "\n",
    "* First, we create a model object.\n",
    "* Second, we add layers to the model one by one.\n",
    "\n",
    "After doing these, we'll end up with a deep learning model structure. The next steps are as follows:\n",
    "\n",
    "* We define an **optimizer** and compile our model.\n",
    "* After compiling the model, we train our model using training data.\n",
    "* The last step is to evaluate the performance of our model on a test set.\n",
    "\n",
    "The figure below sketches the steps to implement a deep learning model in Keras:\n",
    "\n",
    "![keras](assets/keras.png)\n",
    "\n",
    "That is all! Let's start by talking about the dataset we'll be using in this checkpoint. Then, we'll implement our model using Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Throughout this module, we'll be using a famous optical character recognition (OCR) dataset called **MNIST**. This dataset comprises of 70000 grayscale images of handwritten digits. Using this dataset in deep learning research and education is classical and that's why we choose it here. In the following, we'll load the dataset and do some data preprocessing. As we'll see shortly, each image is represented as 28x28 pixel data. This is a two-dimensional vector. We'll first convert this to a vector of 784 length which will be single-dimensional. We also normalize each vector by dividing each element by 255 (this is the maximum value of the RGB color scale). \n",
    "\n",
    "We load the MNIST dataset using Keras' `datasets` module. We use `mnist` class from this module to load the MNIST data. In order to do that, we need to import it as follows:\n",
    "\n",
    "**Note**: You don't have to use this method to download the MNIST dataset. It's available online [here](http://yann.lecun.com/exdb/mnist/). You can also download the dataset from that link and then load it your own way. Notice that the dataset in the link is separated into two (as training and test sets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from tensorflow.keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we load the data and do our preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "input_dim = 784  # 28*28\n",
    "output_dim = nb_classes = 10\n",
    "batch_size = 128\n",
    "nb_epoch = 20\n",
    "print (X_train.shape)\n",
    "X_train = X_train.reshape(60000, input_dim)\n",
    "X_test = X_test.reshape(10000, input_dim)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we one hot code our target variable using `to_categorical` function of Keras' `utils` module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "Y_train = to_categorical(y_train, nb_classes)\n",
    "Y_test = to_categorical(y_test, nb_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's check the size of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you see, the size of each image is 784. In fact, all images in MNIST are 28 by 28 pixels and 784 is just the result of the multiplication of 28 by 28. So, the data we have is a flattened version of the images where each row in the 28x28 matrix is concatenated side by side. \n",
    "\n",
    "Let's plot some images and see what they look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 2000x500 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(20,5))\n",
    "\n",
    "plt.subplot(141)\n",
    "plt.imshow(X_train[123].reshape(28,28), cmap=\"gray\")\n",
    "plt.title(\"Label of the image: {}\".format(y_train[123]))\n",
    "\n",
    "plt.subplot(142)\n",
    "plt.imshow(X_train[124].reshape(28,28), cmap=\"gray\")\n",
    "plt.title(\"Label of the image: {}\".format(y_train[124]))\n",
    "\n",
    "plt.subplot(143)\n",
    "plt.imshow(X_train[125].reshape(28,28), cmap=\"gray\")\n",
    "plt.title(\"Label of the image: {}\".format(y_train[125]))\n",
    "\n",
    "plt.subplot(144)\n",
    "plt.imshow(X_train[126].reshape(28,28), cmap=\"gray\")\n",
    "plt.title(\"Label of the image: {}\".format(y_train[126]))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now ready to jump into building our ANN model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the model\n",
    "\n",
    "As we said before, we'll build our model using `Sequential` class of Keras' `models` module. Once we create our model as:\n",
    "```python\n",
    "model = Sequential()\n",
    "```\n",
    "We'll start to add layers to our model object one by one (that is, sequentially). The layer type we'll use is called the **dense** layer which we'll import from the `layers` module of the Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "model = Sequential()\n",
    "# our first dense layer\n",
    "model.add(Dense(1028, input_shape=(784,), activation=\"relu\"))\n",
    "# our second dense layer\n",
    "model.add(Dense(1028, activation=\"relu\"))\n",
    "# last layer is the output layer.\n",
    "model.add(Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that, we set the neuron size of the output layer to 10. This is because in MNIST there are 10 classes. We also set the activation function of the output layer to **softmax**. We'll discuss why we use softmax as the activation function in the output layer. For now, we say that when we give an image as input to the model, our model will produce 10 probabilities for each of the 10 classes in the MNIST data. The largest probability class will be the prediction of the model.\n",
    "\n",
    "We can have a look at the structure of our ANN model using the `summary()` method of our model object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you see, we have three dense layers of which the last one is the output layer. In total, we have 1,875,082 parameters to be estimated in our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling the model\n",
    "\n",
    "Now we can compile our model. When compiling the model, we define three things:\n",
    "\n",
    "1. The optimizer that will be used in the training. If you don't know about the optimizers in deep learning, do not worry. We just use it in this checkpoint. But, we'll talk about them in the following checkpoints.\n",
    "2. The loss function. It's necessary to specify a loss function for a model. Training algorithms use this loss function and try to minimize it during the training. This is also something we'll cover in the next checkpoint.\n",
    "3. The metric to measure the training performance of our model. In this example, we use the accuracy metric, since our task is a classification task and our dataset is a balanced one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='sgd', loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "We're now ready to train our model. Training a model in Keras is done by calling the `fit()` method of the model object. In the following, we train our model:\n",
    "\n",
    "* Using 128 as the **batch size**. This is something we'll discuss in a later checkpoint.\n",
    "* Using 20 as the number of epochs. In deep learning jargon, **epoch** means full use of all of the examples in the training data during the training the model. So, we'll train our model during 20 epochs, that's we'll use all of the observations in our training data 20 times when training our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# setting verbose=1 prints out some results after each epoch\n",
    "model.fit(X_train, Y_train, batch_size=batch_size, epochs=20, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow! Our model achieved almost 97% accuracy in the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the model\n",
    "\n",
    "The last step is to evaluate our model using the test set we set apart before. For this purpose, we use the `evaluate()` method of the model object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test set accuracy of our model is almost 97%. Good job. Now it's your turn!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
