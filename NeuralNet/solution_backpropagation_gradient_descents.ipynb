{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "latex_envs": {
      "LaTeX_envs_menu_present": true,
      "autoclose": false,
      "autocomplete": true,
      "bibliofile": "biblio.bib",
      "cite_by": "apalike",
      "current_citInitial": 1,
      "eqLabelWithNumbers": true,
      "eqNumInitial": 1,
      "hotkeys": {
        "equation": "Ctrl-E",
        "itemize": "Ctrl-I"
      },
      "labels_anchors": false,
      "latex_user_defs": false,
      "report_style_numbering": false,
      "user_envs_cfg": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "name": "5.solution_backpropagation_gradient_descents.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cD_9z3-vZAGG",
        "colab_type": "text"
      },
      "source": [
        "# Solutions for checkpoint 5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hecMfipVZAGV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential \n",
        "from tensorflow.keras.layers import Dense"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uNCtePuXZAGz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "input_dim = 784  # 28*28\n",
        "output_dim = nb_classes = 10\n",
        "nb_epoch = 20\n",
        "\n",
        "X_train = X_train.reshape(60000, input_dim)\n",
        "X_test = X_test.reshape(10000, input_dim)\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "X_train /= 255\n",
        "X_test /= 255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejIY_JPBZAHH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Y_train = to_categorical(y_train, nb_classes)\n",
        "Y_test = to_categorical(y_test, nb_classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rjv91AoQZAHZ",
        "colab_type": "text"
      },
      "source": [
        "## 1. In this task, you'll implement several ANN models with different batch sizes. Specifically:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obUMfxNYZAHi",
        "colab_type": "text"
      },
      "source": [
        "### Implement a three layer ANN model with 128, 64 and 10 neurons in the layers using 8 as the mini batch size.\n",
        "   "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jsbuPUgZAHo",
        "colab_type": "code",
        "colab": {},
        "outputId": "bd3d80cb-e3e9-4b91-e549-662243632d47"
      },
      "source": [
        "model = Sequential()\n",
        "# our first dense layer\n",
        "model.add(Dense(128, input_shape=(784,), activation=\"relu\"))\n",
        "# our second dense layer\n",
        "model.add(Dense(64, activation=\"relu\"))\n",
        "# last layer is the output layer.\n",
        "model.add(Dense(10, activation=\"softmax\"))\n",
        "\n",
        "model.compile(optimizer='sgd', loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# setting verbose=1 prints out some results after each epoch\n",
        "model.fit(X_train, Y_train, batch_size=8, epochs=20, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /Users/mladmin/miniconda3/envs/datascience/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /Users/mladmin/miniconda3/envs/datascience/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Epoch 1/20\n",
            "60000/60000 [==============================] - 11s 187us/sample - loss: 0.3736 - acc: 0.8938\n",
            "Epoch 2/20\n",
            "60000/60000 [==============================] - 11s 182us/sample - loss: 0.1741 - acc: 0.9491\n",
            "Epoch 3/20\n",
            "60000/60000 [==============================] - 10s 172us/sample - loss: 0.1271 - acc: 0.9633\n",
            "Epoch 4/20\n",
            "60000/60000 [==============================] - 11s 178us/sample - loss: 0.1011 - acc: 0.9698\n",
            "Epoch 5/20\n",
            "60000/60000 [==============================] - 10s 163us/sample - loss: 0.0828 - acc: 0.9753\n",
            "Epoch 6/20\n",
            "60000/60000 [==============================] - 10s 166us/sample - loss: 0.0696 - acc: 0.9793\n",
            "Epoch 7/20\n",
            "60000/60000 [==============================] - 10s 165us/sample - loss: 0.0585 - acc: 0.9826\n",
            "Epoch 8/20\n",
            "60000/60000 [==============================] - 10s 164us/sample - loss: 0.0502 - acc: 0.9849\n",
            "Epoch 9/20\n",
            "60000/60000 [==============================] - 10s 163us/sample - loss: 0.0430 - acc: 0.9877\n",
            "Epoch 10/20\n",
            "60000/60000 [==============================] - 10s 161us/sample - loss: 0.0380 - acc: 0.9887\n",
            "Epoch 11/20\n",
            "60000/60000 [==============================] - 10s 161us/sample - loss: 0.0322 - acc: 0.9906\n",
            "Epoch 12/20\n",
            "60000/60000 [==============================] - 10s 164us/sample - loss: 0.0279 - acc: 0.9919\n",
            "Epoch 13/20\n",
            "60000/60000 [==============================] - 10s 163us/sample - loss: 0.0236 - acc: 0.9934\n",
            "Epoch 14/20\n",
            "60000/60000 [==============================] - 10s 167us/sample - loss: 0.0205 - acc: 0.9946\n",
            "Epoch 15/20\n",
            "60000/60000 [==============================] - 10s 164us/sample - loss: 0.0177 - acc: 0.9958\n",
            "Epoch 16/20\n",
            "60000/60000 [==============================] - 10s 169us/sample - loss: 0.0149 - acc: 0.9966\n",
            "Epoch 17/20\n",
            "60000/60000 [==============================] - 11s 177us/sample - loss: 0.0131 - acc: 0.9970\n",
            "Epoch 18/20\n",
            "60000/60000 [==============================] - 10s 171us/sample - loss: 0.0111 - acc: 0.9978\n",
            "Epoch 19/20\n",
            "60000/60000 [==============================] - 10s 174us/sample - loss: 0.0095 - acc: 0.9981\n",
            "Epoch 20/20\n",
            "60000/60000 [==============================] - 11s 185us/sample - loss: 0.0081 - acc: 0.9987\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x13c0045c0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nfJE-4MbZAH6",
        "colab_type": "code",
        "colab": {},
        "outputId": "f50ea234-7da5-4e87-a312-f61e351ea728"
      },
      "source": [
        "score = model.evaluate(X_test, Y_test, verbose=0)\n",
        "print('Test score:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test score: 0.07625642182910597\n",
            "Test accuracy: 0.9785\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyMCAfXwZAIK",
        "colab_type": "text"
      },
      "source": [
        "### Implement a three layer ANN model with 128, 64 and 10 neurons in the layers using 128 as the mini batch size.\n",
        "   "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdKTqvXWZAIO",
        "colab_type": "code",
        "colab": {},
        "outputId": "406ffa0b-d07d-4042-997c-875bdbc981f3"
      },
      "source": [
        "model = Sequential()\n",
        "# our first dense layer\n",
        "model.add(Dense(128, input_shape=(784,), activation=\"relu\"))\n",
        "# our second dense layer\n",
        "model.add(Dense(64, activation=\"relu\"))\n",
        "# last layer is the output layer.\n",
        "model.add(Dense(10, activation=\"softmax\"))\n",
        "\n",
        "model.compile(optimizer='sgd', loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# setting verbose=1 prints out some results after each epoch\n",
        "model.fit(X_train, Y_train, batch_size=128, epochs=20, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 1.2831 - acc: 0.68180s - loss: 1.6138 - ac\n",
            "Epoch 2/20\n",
            "60000/60000 [==============================] - 1s 18us/sample - loss: 0.4964 - acc: 0.8698\n",
            "Epoch 3/20\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.3820 - acc: 0.8945\n",
            "Epoch 4/20\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.3350 - acc: 0.9057\n",
            "Epoch 5/20\n",
            "60000/60000 [==============================] - 1s 18us/sample - loss: 0.3066 - acc: 0.9135\n",
            "Epoch 6/20\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.2860 - acc: 0.9187\n",
            "Epoch 7/20\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.2696 - acc: 0.9224\n",
            "Epoch 8/20\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.2560 - acc: 0.9265\n",
            "Epoch 9/20\n",
            "60000/60000 [==============================] - 1s 18us/sample - loss: 0.2441 - acc: 0.9299\n",
            "Epoch 10/20\n",
            "60000/60000 [==============================] - 1s 18us/sample - loss: 0.2338 - acc: 0.9330\n",
            "Epoch 11/20\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.2243 - acc: 0.9359\n",
            "Epoch 12/20\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.2157 - acc: 0.9382\n",
            "Epoch 13/20\n",
            "60000/60000 [==============================] - 1s 18us/sample - loss: 0.2079 - acc: 0.9404\n",
            "Epoch 14/20\n",
            "60000/60000 [==============================] - 1s 18us/sample - loss: 0.2008 - acc: 0.9427\n",
            "Epoch 15/20\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 0.1940 - acc: 0.9445\n",
            "Epoch 16/20\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 0.1876 - acc: 0.9466\n",
            "Epoch 17/20\n",
            "60000/60000 [==============================] - 1s 22us/sample - loss: 0.1818 - acc: 0.9483\n",
            "Epoch 18/20\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.1763 - acc: 0.9498\n",
            "Epoch 19/20\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.1712 - acc: 0.9514\n",
            "Epoch 20/20\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 0.1661 - acc: 0.95270s - loss: 0.1704\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x13c82aef0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjN7cQ9OZAIe",
        "colab_type": "code",
        "colab": {},
        "outputId": "9ca994cd-bbf3-455f-f7c9-c0def1f1bc0b"
      },
      "source": [
        "score = model.evaluate(X_test, Y_test, verbose=0)\n",
        "print('Test score:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test score: 0.16599947365522386\n",
            "Test accuracy: 0.9518\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TcaP3y5LZAIt",
        "colab_type": "text"
      },
      "source": [
        "### Implement a three layer ANN model with 128, 64 and 10 neurons in the layers using the full sample as the batch.\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t5rBwAKzZAIw",
        "colab_type": "code",
        "colab": {},
        "outputId": "d5d26fa8-f2df-4384-b8a8-020354ea7493"
      },
      "source": [
        "model = Sequential()\n",
        "# our first dense layer\n",
        "model.add(Dense(128, input_shape=(784,), activation=\"relu\"))\n",
        "# our second dense layer\n",
        "model.add(Dense(64, activation=\"relu\"))\n",
        "# last layer is the output layer.\n",
        "model.add(Dense(10, activation=\"softmax\"))\n",
        "\n",
        "model.compile(optimizer='sgd', loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# setting verbose=1 prints out some results after each epoch\n",
        "model.fit(X_train, Y_train, batch_size=X_train.shape[0], epochs=20, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 2.3095 - acc: 0.1142\n",
            "Epoch 2/20\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 2.3032 - acc: 0.1173\n",
            "Epoch 3/20\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 2.2970 - acc: 0.1199\n",
            "Epoch 4/20\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 2.2909 - acc: 0.1233\n",
            "Epoch 5/20\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 2.2849 - acc: 0.1264\n",
            "Epoch 6/20\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 2.2789 - acc: 0.1298\n",
            "Epoch 7/20\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 2.2731 - acc: 0.1330\n",
            "Epoch 8/20\n",
            "60000/60000 [==============================] - 1s 10us/sample - loss: 2.2673 - acc: 0.1367\n",
            "Epoch 9/20\n",
            "60000/60000 [==============================] - 1s 10us/sample - loss: 2.2616 - acc: 0.1404\n",
            "Epoch 10/20\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 2.2560 - acc: 0.1450\n",
            "Epoch 11/20\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 2.2504 - acc: 0.1498\n",
            "Epoch 12/20\n",
            "60000/60000 [==============================] - 1s 10us/sample - loss: 2.2449 - acc: 0.1543\n",
            "Epoch 13/20\n",
            "60000/60000 [==============================] - 1s 10us/sample - loss: 2.2394 - acc: 0.1592\n",
            "Epoch 14/20\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 2.2339 - acc: 0.1640\n",
            "Epoch 15/20\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 2.2286 - acc: 0.1691\n",
            "Epoch 16/20\n",
            "60000/60000 [==============================] - 1s 10us/sample - loss: 2.2232 - acc: 0.1743\n",
            "Epoch 17/20\n",
            "60000/60000 [==============================] - 1s 10us/sample - loss: 2.2179 - acc: 0.1798\n",
            "Epoch 18/20\n",
            "60000/60000 [==============================] - 1s 13us/sample - loss: 2.2126 - acc: 0.1854\n",
            "Epoch 19/20\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 2.2073 - acc: 0.1909\n",
            "Epoch 20/20\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 2.2020 - acc: 0.1971\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x1308a39e8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71Yz3tkDZAI_",
        "colab_type": "code",
        "colab": {},
        "outputId": "8810894d-26b2-4963-b196-d99e504a341f"
      },
      "source": [
        "score = model.evaluate(X_test, Y_test, verbose=0)\n",
        "print('Test score:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test score: 2.1876832405090334\n",
            "Test accuracy: 0.2145\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3aAA7G5qZAJS",
        "colab_type": "text"
      },
      "source": [
        "### Compare the result of each model with each other. Which batch size did perform better?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uoxyGnrMZAJV",
        "colab_type": "text"
      },
      "source": [
        "The best performance both in training and test sets are achieved using 8 as the mini batch size. However, the difference between the scores of the training and test sets are relatively large compared to the scores achieved when using 128 as mini batch size. Since the scores achieved when using 128 as the mini batch size are close to those that are achieved when using 8 as the mini batch size, one can go with 128 because of the overfitting concerns. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMiXlFDQZAJb",
        "colab_type": "text"
      },
      "source": [
        "## 2. In this task, you'll implement several ANN models with different learning rates for the stochastic gradient descent. In all of the models below, use 128 as your mini batch size.\n",
        "   "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8pMLP9FGZAJe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras import optimizers\n",
        "sgd_001 = optimizers.SGD(lr=0.01)\n",
        "sgd_100 = optimizers.SGD(lr=100)\n",
        "sgd_00000001 = optimizers.SGD(lr=0.0000001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3A2uyaBzZAJn",
        "colab_type": "text"
      },
      "source": [
        "### Implement a three layer ANN model with 128, 64 and 10 neurons in the layers using 0.01 as the learning rate.\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCvBdyNyZAJt",
        "colab_type": "code",
        "colab": {},
        "outputId": "d49a4d70-9a1a-48a5-b420-db5248f45524"
      },
      "source": [
        "model = Sequential()\n",
        "# our first dense layer\n",
        "model.add(Dense(128, input_shape=(784,), activation=\"relu\"))\n",
        "# our second dense layer\n",
        "model.add(Dense(64, activation=\"relu\"))\n",
        "# last layer is the output layer.\n",
        "model.add(Dense(10, activation=\"softmax\"))\n",
        "\n",
        "model.compile(optimizer=sgd_001, loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# setting verbose=1 prints out some results after each epoch\n",
        "model.fit(X_train, Y_train, batch_size=128, epochs=20, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 1.2218 - acc: 0.6984\n",
            "Epoch 2/20\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.4829 - acc: 0.8751\n",
            "Epoch 3/20\n",
            "60000/60000 [==============================] - 1s 18us/sample - loss: 0.3820 - acc: 0.8941\n",
            "Epoch 4/20\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.3393 - acc: 0.9045\n",
            "Epoch 5/20\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.3124 - acc: 0.9117\n",
            "Epoch 6/20\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 0.2925 - acc: 0.9167\n",
            "Epoch 7/20\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.2765 - acc: 0.9215\n",
            "Epoch 8/20\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 0.2630 - acc: 0.9252\n",
            "Epoch 9/20\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 0.2507 - acc: 0.9284\n",
            "Epoch 10/20\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 0.2400 - acc: 0.9315\n",
            "Epoch 11/20\n",
            "60000/60000 [==============================] - 1s 22us/sample - loss: 0.2303 - acc: 0.9345\n",
            "Epoch 12/20\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 0.2214 - acc: 0.9370\n",
            "Epoch 13/20\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.2134 - acc: 0.9390\n",
            "Epoch 14/20\n",
            "60000/60000 [==============================] - 1s 18us/sample - loss: 0.2060 - acc: 0.9414\n",
            "Epoch 15/20\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 0.1991 - acc: 0.9433\n",
            "Epoch 16/20\n",
            "60000/60000 [==============================] - 1s 18us/sample - loss: 0.1927 - acc: 0.9450\n",
            "Epoch 17/20\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 0.1867 - acc: 0.9466\n",
            "Epoch 18/20\n",
            "60000/60000 [==============================] - 1s 22us/sample - loss: 0.1811 - acc: 0.9481\n",
            "Epoch 19/20\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.1757 - acc: 0.9497\n",
            "Epoch 20/20\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 0.1707 - acc: 0.9512\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x13d23c5c0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MuoewsXoZAJ3",
        "colab_type": "code",
        "colab": {},
        "outputId": "c8c6b62a-41df-4802-b016-d7f073c4be78"
      },
      "source": [
        "score = model.evaluate(X_test, Y_test, verbose=0)\n",
        "print('Test score:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test score: 0.17147365436702966\n",
            "Test accuracy: 0.9512\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jL8vXBPXZAKC",
        "colab_type": "text"
      },
      "source": [
        "### Implement a three layer ANN model with 128, 64 and 10 neurons in the layers using 100 as the learning rate.\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NNGmr4nGZAKD",
        "colab_type": "code",
        "colab": {},
        "outputId": "cf77fbc8-b015-4dbf-cac3-345e33e4c895"
      },
      "source": [
        "model = Sequential()\n",
        "# our first dense layer\n",
        "model.add(Dense(128, input_shape=(784,), activation=\"relu\"))\n",
        "# our second dense layer\n",
        "model.add(Dense(64, activation=\"relu\"))\n",
        "# last layer is the output layer.\n",
        "model.add(Dense(10, activation=\"softmax\"))\n",
        "\n",
        "model.compile(optimizer=sgd_100, loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# setting verbose=1 prints out some results after each epoch\n",
        "model.fit(X_train, Y_train, batch_size=128, epochs=20, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 14.5224 - acc: 0.0975\n",
            "Epoch 2/20\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 14.5463 - acc: 0.0975\n",
            "Epoch 3/20\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 14.5463 - acc: 0.0975\n",
            "Epoch 4/20\n",
            "60000/60000 [==============================] - 1s 18us/sample - loss: 14.5463 - acc: 0.0975\n",
            "Epoch 5/20\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 14.5463 - acc: 0.0975\n",
            "Epoch 6/20\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 14.5463 - acc: 0.0975\n",
            "Epoch 7/20\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 14.5463 - acc: 0.0975\n",
            "Epoch 8/20\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 14.5463 - acc: 0.0975\n",
            "Epoch 9/20\n",
            "60000/60000 [==============================] - 1s 18us/sample - loss: 14.5463 - acc: 0.0975\n",
            "Epoch 10/20\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 14.5463 - acc: 0.0975\n",
            "Epoch 11/20\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 14.5463 - acc: 0.0975\n",
            "Epoch 12/20\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 14.5463 - acc: 0.0975\n",
            "Epoch 13/20\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 14.5463 - acc: 0.0975\n",
            "Epoch 14/20\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 14.5463 - acc: 0.0975\n",
            "Epoch 15/20\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 14.5463 - acc: 0.0975\n",
            "Epoch 16/20\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 14.5463 - acc: 0.0975\n",
            "Epoch 17/20\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 14.5463 - acc: 0.0975\n",
            "Epoch 18/20\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 14.5463 - acc: 0.0975\n",
            "Epoch 19/20\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 14.5463 - acc: 0.0975\n",
            "Epoch 20/20\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 14.5463 - acc: 0.0975\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x13da0e2e8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WiQt9AATZAKJ",
        "colab_type": "code",
        "colab": {},
        "outputId": "03e077cc-206b-48d1-d1f8-303b09fa3801"
      },
      "source": [
        "score = model.evaluate(X_test, Y_test, verbose=0)\n",
        "print('Test score:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test score: 14.548192700195312\n",
            "Test accuracy: 0.0974\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2NWIu7GZAKR",
        "colab_type": "text"
      },
      "source": [
        "### Implement a three layer ANN model with 128, 64 and 10 neurons in the layers using 0.0000001 as the learning rate.\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dsrlrLpZAKS",
        "colab_type": "code",
        "colab": {},
        "outputId": "2d760039-9f2d-456d-eff3-55cb3f22968b"
      },
      "source": [
        "model = Sequential()\n",
        "# our first dense layer\n",
        "model.add(Dense(128, input_shape=(784,), activation=\"relu\"))\n",
        "# our second dense layer\n",
        "model.add(Dense(64, activation=\"relu\"))\n",
        "# last layer is the output layer.\n",
        "model.add(Dense(10, activation=\"softmax\"))\n",
        "\n",
        "model.compile(optimizer=sgd_00000001, loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# setting verbose=1 prints out some results after each epoch\n",
        "model.fit(X_train, Y_train, batch_size=128, epochs=20, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "60000/60000 [==============================] - 1s 22us/sample - loss: 2.3366 - acc: 0.1172\n",
            "Epoch 2/20\n",
            "60000/60000 [==============================] - 1s 18us/sample - loss: 2.3366 - acc: 0.1172\n",
            "Epoch 3/20\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 2.3366 - acc: 0.1172\n",
            "Epoch 4/20\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 2.3366 - acc: 0.1172\n",
            "Epoch 5/20\n",
            "60000/60000 [==============================] - 1s 18us/sample - loss: 2.3366 - acc: 0.1172\n",
            "Epoch 6/20\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 2.3366 - acc: 0.11730s - loss: 2.3366 - acc: 0.117\n",
            "Epoch 7/20\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 2.3366 - acc: 0.1173\n",
            "Epoch 8/20\n",
            "60000/60000 [==============================] - 1s 18us/sample - loss: 2.3365 - acc: 0.1173\n",
            "Epoch 9/20\n",
            "60000/60000 [==============================] - 1s 18us/sample - loss: 2.3365 - acc: 0.1173\n",
            "Epoch 10/20\n",
            "60000/60000 [==============================] - 1s 18us/sample - loss: 2.3365 - acc: 0.1173\n",
            "Epoch 11/20\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 2.3365 - acc: 0.1173\n",
            "Epoch 12/20\n",
            "60000/60000 [==============================] - 1s 18us/sample - loss: 2.3365 - acc: 0.1173\n",
            "Epoch 13/20\n",
            "60000/60000 [==============================] - 1s 22us/sample - loss: 2.3365 - acc: 0.1173\n",
            "Epoch 14/20\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 2.3365 - acc: 0.1173\n",
            "Epoch 15/20\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 2.3364 - acc: 0.1173\n",
            "Epoch 16/20\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 2.3364 - acc: 0.1173\n",
            "Epoch 17/20\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 2.3364 - acc: 0.1173\n",
            "Epoch 18/20\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 2.3364 - acc: 0.1173\n",
            "Epoch 19/20\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 2.3364 - acc: 0.1173\n",
            "Epoch 20/20\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 2.3364 - acc: 0.1173\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x13dd71b00>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ErJUW3BZAKh",
        "colab_type": "code",
        "colab": {},
        "outputId": "d43ac913-47f8-493d-a375-f4964bbf4bb7"
      },
      "source": [
        "score = model.evaluate(X_test, Y_test, verbose=0)\n",
        "print('Test score:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test score: 2.3359681705474853\n",
            "Test accuracy: 0.1193\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_eXtwnLZAKt",
        "colab_type": "text"
      },
      "source": [
        "### Compare the result of each model with each other. Which learning rate did perform better?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5aOd_UUcZAK0",
        "colab_type": "text"
      },
      "source": [
        "The model converged when using 0.01 as the learning rate. However, it diverged when using 100, because that value deemed to be too large. Using 0.0000001 as the learning rate causes the model to improve very slowly. Hence the accuracy improved very little. So, it deemed to be too low."
      ]
    }
  ]
}
