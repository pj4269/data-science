{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# What is the role of activation functions?\n",
    "\n",
    "**The reason why activation functions are so crucial in deep learning models is that they introduce non-linearity to the models**. For a feed-forward neural network, they are actually the only non-linearity in the model. To understand what if we don't use a non-linear activation function, first remember how we apply activation function to the output of the linear transformation in a neuron:\n",
    "\n",
    "![single neuron](assets/single_neuron.png)\n",
    "\n",
    "As depicted in the figure above, a neuron first sums up each element of the input by multiplying their associated weights. This is just a linear transformation of the input:\n",
    "\n",
    "$$y=b+x_1*w_1+x_2*w_2+...+x_n*w_n$$\n",
    "\n",
    "The activation functions come into play after this linear transformation. A neuron gives the result of the previous summation to an activation function and returns the result of that activation function:\n",
    "\n",
    "$$Activation\\_fn(y) = Activation\\_fn(b+x_1*w_1+x_2*w_2+...+x_n*w_n)$$\n",
    "\n",
    "n represents the size of the input vector (or the number of features), $x_1,...,x_n$ represents the values in the input vector of size n and $b$ represents the bias term. $w_1,...,w_n$ represents the weights of the neuron. **$b$ and $w_i$'s are the parameters to be estimated**.\n",
    "\n",
    "To better understand the role of the activation functions, assume that neurons in a layer just output the results of the linear summations. These will be the inputs of the next layer. Again assume that the neurons of the next layer also output the results of their linear summations. Hence, the output of a neuron in this second layer would be: \n",
    "\n",
    "$$y_2 = b + o_1*w_1+o_2*w_2+...+o_n*w_n$$\n",
    "\n",
    "where $o_1, o_2,..., o_n$ denotes the outputs of the neurons of the previous layer. If we substitute $o_1, o_2,..., o_n$ like $o_1 = b+x_1*w_1+x_2*w_2+...+x_n*w_n$, then we end up that the output of that neuron will be something like:\n",
    "\n",
    "$$y_2=b_2+x_1*k_1*w_1+x_2*k_2*w_2+...+x_n*k_n*w_n$$\n",
    "\n",
    "This means that if we update the weights of the neuron from $w_1$ to $k_1*w_1$, then we can say that the neurons from the second layer also accepts the original inputs of the previous layer. As a result, the first layer is unnecessary! In short, the neurons in a layer just spit out the linear transformation of its input to the next layer and the next layer does the same thing so on and so forth. In the end, the final result can be rewritten as the output of a single layer! **Hence, activation functions enable deep neural networks to have stacked layers and this is why they are so crucial**. In a nutshell:\n",
    "\n",
    "> If we don't use non-linear activation functions in our neurons, then stacking multiple layers on top of each other becomes something unnecessary and all the layers can be collapsed to a single layer. Thus, we can only talk about deep learning as long as we use non-linear activation functions in the neurons.\n",
    "\n",
    "Next, let's review some common activation functions.\n",
    "\n",
    "# Activation functions\n",
    "\n",
    "There are many activation functions out there and the research for discovering new ones is a hot topic in the deep learning literature. Below, we'll mention some popular activation functions that found common applicability in the real-world tasks. The figure below depicts some of them along with their mathematical formulas and how they look like graphically:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![activation functions](assets/activation_functions.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's briefly talk about them one by one:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid\n",
    "\n",
    "You might already remember the sigmoid function from the binary logistic regression. This historically popular function is believed to mimic the activation in our biological brains. Mathematically, sigmoid function is: \n",
    "\n",
    "$$\\sigma(x)=1+\\dfrac{1}{e^{-x}}$$\n",
    "\n",
    "Though it's one of the popular activation functions that is used in the deep learning models, its popularity is diminishing because of:\n",
    "\n",
    "* It's non-zero centered: It results in zig-zags during optimization.\n",
    "* It results in vanishing gradient or saturation: For high values of input values, the learning becomes very very slow if anything because the gradient (derivatives) becomes very very close to zero! We'll talk about gradients in the following checkpoint.\n",
    "* Because of the vanishing gradient, a careful initialization of the weights of the network is important. But finding good enough values for initializing the parameters isn't trivial every time.\n",
    "\n",
    "In Keras, we can define sigmoid as the activation function of a dense layer as follows:\n",
    "\n",
    "```python\n",
    "Dense(1028, input_shape=(784,), activation=\"sigmoid\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperbolic tangent (tanh)\n",
    "\n",
    "Hypberbolic tangent is another popular activation function. It's actually a scaled version of sigmoid:\n",
    "\n",
    "$$tanh(x)=2\\sigma(2x)-1$$\n",
    "\n",
    "It's zero centered as opposed to sigmoid. However, it also saturates and results in vanishing gradient problem.\n",
    "\n",
    "In Keras, we can define hyperbolic tangent as the activation function of a dense layer as follows:\n",
    "\n",
    "```python\n",
    "Dense(1028, input_shape=(784,), activation=\"tanh\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rectified linear units (ReLU)\n",
    "\n",
    "Rectified linear unit is probably the most common activation function in deep learning literature. If you recall, we also used it in the layers of the model we implemented in the previous checkpoint. Some properties of the ReLU are as follows:\n",
    "\n",
    "* It cuts off values below zero.\n",
    "* It's non-saturating.\n",
    "* It enables models to converge faster than sigmoid and tanh.\n",
    "* It's easy to implement.\n",
    "* However, using ReLU in the networks may cause some neurons to irreversibly die! If learning rate set too high, as much as %40 of the neurons can die.\n",
    "\n",
    "In Keras, we can define ReLU as the activation function of a dense layer as follows:\n",
    "\n",
    "```python\n",
    "Dense(1028, input_shape=(784,), activation=\"relu\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leaky ReLU\n",
    "\n",
    "Leaky ReLu is proposed to solve the dying neurons problem when using ReLU. Usually, we set the slope for the negative values to be 0.1. That being said, there are other variants of the leaky ReLU:\n",
    "\n",
    "* A variant of leaky ReLU is called **parametric ReLU (PReLU)**. In PReLU, the slope of the negative part is a parameter to learn.\n",
    "* **MaxOut**: $max(a_1x+b_1,a_2x+b_2)$. Namely, two parametric slopes. ReLU, leaky ReLU and PReLU are special cases of MaxOut. It solves the problems associated with the formers, but in this case, the parameters to learn double!\n",
    "\n",
    "In Keras, leaky ReLU is available in the `layers` module instead of the `activations` as opposed to the other activation functions we talked about previously. If you want to use leaky ReLU as the activation function for a layer, then you should add it as a layer like the following:\n",
    "\n",
    "```python\n",
    "model.add(Dense(1028, input_shape=(784,)))\n",
    "model.add(keras.layers.LeakyReLU(alpha=0.3))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax\n",
    "\n",
    "The last activation function we want to discuss is the famous softmax function. You may already recall softmax from the multiclass logistic regression. Using softmax in output layer of a deep neural network is quite common in classification tasks. What makes sigmoid very popular in the output layer is that its outputs can be read as the probabilities. So, almost always you want to add softmax as the activation of the output layer when implementing classification models.\n",
    "\n",
    "Mathematically, softmax is:\n",
    "\n",
    "$$\\frac{\\exp^{-x_i}}{\\sum{\\exp^{-x_j}}}$$\n",
    "\n",
    "In Keras, we can define softmax as the activation function of a dense layer as follows:\n",
    "\n",
    "```python\n",
    "Dense(10, input_shape=(784,), activation=\"softmax\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras' activation module\n",
    "\n",
    "Before closing our discussion of activation functions, let's give a brief treatment of `activations` module of Keras. In Keras, activation functions can either be used through an Activation layer, or through the activation argument supported by all forward layers:\n",
    "\n",
    "```python\n",
    "from tensorflow.keras.layers import Activation, Dense\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('tanh'))\n",
    "```\n",
    "\n",
    "This is equivalent to:\n",
    "\n",
    "```python\n",
    "model.add(Dense(64, activation='tanh'))\n",
    "```\n",
    "\n",
    "If you're using Keras as standalone instead of through TensorFlow, you can also pass an element-wise TensorFlow/Theano/CNTK function as an activation:\n",
    "\n",
    "```python\n",
    "from keras import backend as K\n",
    "model.add(Dense(64, activation=K.tanh))\n",
    "```\n",
    "\n",
    "Activations that are more complex (eg. learnable activations, which maintain a state) are available as advanced activation layers, and can be found in the module `tf.keras.layers`. These include `PReLU` and `LeakyReLU`. You can find more in [Keras documentation](https://keras.io/layers/advanced-activations/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss functions\n",
    "\n",
    "So far in this checkpoint, we saw several activation functions. In a typical neural network, we have millions of parameters to determine. So, the natural next step in our discussion is to figure out how we find the best parameters in our neural networks.\n",
    "\n",
    "The methods that we make use of to find out the best parameters belong to the territory of **optimization**. As we'll see later in this module, the most commonly used optimization algorithms are the variants of the **Gradient Descent** algorithm. Here we introduce the **loss functions (or cost functions)** that quantify how well our models perform. Then in the next checkpoint, we'll see how we use them to train a neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are the loss functions\n",
    "\n",
    "> <font color=green>\"The function we want to minimize or maximize is called the objective function or criterion. When we are minimizing it, we may also call it the cost function, loss function, or error function.\"</font> - Deep Learning, 2016, p. 82\n",
    "\n",
    "Loss functions enable us to measure how well our model is doing. Since we're quantifying the performance of our models, we need to choose our loss functions appropriately. This is why we have different loss functions that can do better than each other in specific kinds of tasks. Since the result of a model is produced at the last layer of a neural network, the losses of a model are calculated by feeding the outputs of the model into the associated loss function. In Keras, we define the loss when we're compiling our models:\n",
    "\n",
    "```python\n",
    "from tensorflow.keras import losses\n",
    "# we can provide the loss as string\n",
    "model.compile(optimizer='sgd', \n",
    "              loss='categorical_crossentropy')\n",
    "\n",
    "# or we can provide the class\n",
    "model.compile(optimizer='sgd', \n",
    "              loss=losses.categorical_crossentropy)\n",
    "```\n",
    "Generally speaking, we can categorize loss functions into two as those that are for classification models and those that are for regression models. Now, let's talk about each of them separately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss functions for classification\n",
    "\n",
    "In classification tasks, the models usually output probabilities for each category as a result of the **softmax** layer (or activation).\n",
    "\n",
    "### Hinge loss\n",
    "\n",
    "In simple terms, the score of the correct category should be greater than sum of scores of all incorrect categories by some safety margin (usually one). And hence hinge loss is used for maximum-margin classification, most notably for support vector machines. Although not differentiable, it’s a convex function which makes it easy to work with usual convex optimizers used in the machine learning domain. Mathematically, hinge loss is defined as:\n",
    "\n",
    "$$\\sum_{j \\neq y_i}max(0, s_j-s_{y_i}+1)$$\n",
    "\n",
    "In Keras, we can use it as follows:\n",
    "\n",
    "```python\n",
    "\n",
    "# for binary classification\n",
    "model.compile(optimizer='sgd', \n",
    "              loss=keras.losses.hinge)\n",
    "\n",
    "# for multiclass classification\n",
    "model.compile(optimizer='sgd', \n",
    "              loss=keras.losses.categorical_hinge)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-entropy loss\n",
    "\n",
    "This is the most common setting for classification problems. Cross-entropy loss increases as the predicted probability diverges from the actual label. Technically, cross-entropy comes from the field of information theory and has the unit of “bits.” It is used to estimate the difference between estimated and predicted probability distributions.\n",
    "\n",
    "When modeling a classification problem where we are interested in mapping input variables to a class label, we can model the problem as predicting the probability of an example belonging to each class. In a binary classification problem, there would be two classes, so we may predict the probability of the example belonging to the first class. In the case of multiclass classification, we can predict a probability for the example belonging to each of the classes.\n",
    "\n",
    "Therefore, we seek a set of model weights that minimize the difference between the model’s predicted probability distribution given the dataset and the distribution of probabilities in the training dataset. This is called the cross-entropy.\n",
    "\n",
    "> <font color=green>\"Most modern neural networks are trained using maximum likelihood. This means that the cost function is […] described as the cross-entropy between the training data and the model distribution.\"</font> - Deep Learning, 2016, p. 178-179\n",
    "\n",
    "Mathematically, cross-entropy is defined as follows:\n",
    "\n",
    "$$\\sum_{i}(y_i log(\\hat{y}_i) + (1-y_i)log(1 - \\hat{y}_i))$$\n",
    "\n",
    "Notice that when actual label is 1 (y(i) = 1), second half of function disappears whereas in case actual label is 0 (y(i) = 0) first half is dropped off. In short, we are just multiplying the log of the actual predicted probability for the ground truth class. An important aspect of this is that cross entropy loss penalizes heavily the predictions that are confident but wrong.\n",
    "\n",
    "In Keras, we can use cross-entropy loss as follows:\n",
    "\n",
    "```python\n",
    "\n",
    "# for one-hot encoded outputs\n",
    "model.compile(optimizer='sgd', \n",
    "              loss=keras.losses.categorical_crossentropy)\n",
    "\n",
    "# for single integer valued outputs\n",
    "model.compile(optimizer='sgd', \n",
    "              loss=keras.losses.sparse_categorical_crossentropy)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss functions for regression\n",
    "\n",
    "The outputs of the regression models are real-valued continuous values. So, we need to have a single value as the output. This is why the last layer of a regression model should include only a single neuron.\n",
    "\n",
    "### Mean squared error\n",
    "\n",
    "As the name suggests, mean square error (MSE) is measured as the average of the squared difference between predictions and actual observations. It’s only concerned with the average magnitude of error irrespective of their direction. However, due to squaring, predictions which are far away from actual values are penalized heavily in comparison to less deviated predictions. Moreover, MSE also has nice mathematical properties which make it easier to calculate gradients.\n",
    "\n",
    "Mathematically, MSE is defined as follows:\n",
    "\n",
    "$$\\frac{\\sum_{i}^{n}(y_i - \\hat{y}_i)^2}{n}$$\n",
    "\n",
    "In Keras, we can use it as the following:\n",
    "```python\n",
    "\n",
    "model.compile(optimizer='sgd', \n",
    "              loss=keras.losses.mean_squared_error)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean absolute error\n",
    "\n",
    "Mean absolute error (MAE), on the other hand, is measured as the average of the sum of absolute differences between predictions and actual observations. Like MSE, this as well measures the magnitude of error without considering their direction. Unlike MSE, MAE needs more complicated tools such as linear programming to compute the gradients. Plus MAE is more robust to outliers since it does not make use of square.\n",
    "\n",
    "Mathematically, MAE is defined as follows:\n",
    "\n",
    "$$\\frac{\\sum_{i}^{n}|y_i - \\hat{y}_i|}{n}$$\n",
    "\n",
    "In Keras, we can use it as the following:\n",
    "```python\n",
    "\n",
    "model.compile(optimizer='sgd', \n",
    "              loss=keras.losses.mean_absolute_error)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
